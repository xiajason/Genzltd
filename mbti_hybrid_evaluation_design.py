#!/usr/bin/env python3
"""
MBTIæ··åˆè¯„ä¼°ç­–ç•¥è®¾è®¡
åˆ›å»ºæ—¶é—´: 2025å¹´10æœˆ4æ—¥
ç‰ˆæœ¬: v1.5 (åä¸­å¸ˆèŒƒå¤§å­¦åˆ›æ–°ç‰ˆ)
åŸºäº: æœ¬åœ°æ ¸å¿ƒ+å¤–éƒ¨å¢å¼ºçš„æ··åˆè¯„ä¼°ç­–ç•¥
ç›®æ ‡: è®¾è®¡çµæ´»çš„æ··åˆè¯„ä¼°ç³»ç»Ÿï¼Œæ”¯æŒæœ¬åœ°ç®—æ³•å’Œå¤–éƒ¨APIå¢å¼º
"""

from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, asdict
from enum import Enum
import json
import re
import asyncio
import aiohttp
from datetime import datetime
import logging


# ==================== è¯„ä¼°ç­–ç•¥æšä¸¾ ====================

class EvaluationStrategy(str, Enum):
    """è¯„ä¼°ç­–ç•¥æšä¸¾"""
    LOCAL_ONLY = "local_only"          # ä»…æœ¬åœ°è¯„ä¼°
    API_ONLY = "api_only"             # ä»…å¤–éƒ¨APIè¯„ä¼°
    HYBRID = "hybrid"                 # æ··åˆè¯„ä¼°
    FALLBACK = "fallback"             # é™çº§è¯„ä¼°


class EvaluationMethod(str, Enum):
    """è¯„ä¼°æ–¹æ³•æšä¸¾"""
    MBTI_STANDARD = "mbti_standard"    # æ ‡å‡†MBTIè¯„ä¼°
    MBTI_SIMPLIFIED = "mbti_simplified" # ç®€åŒ–MBTIè¯„ä¼°
    MBTI_ADVANCED = "mbti_advanced"     # é«˜çº§MBTIè¯„ä¼°
    CUSTOM = "custom"                  # è‡ªå®šä¹‰è¯„ä¼°


class APISource(str, Enum):
    """APIæ¥æºæšä¸¾"""
    JISU_DATA = "jisu_data"            # æé€Ÿæ•°æ®
    WADATA = "wadata"                  # æŒ–æ•°æ®
    ALIYUN = "aliyun"                  # é˜¿é‡Œäº‘
    LOCAL = "local"                    # æœ¬åœ°ç®—æ³•


# ==================== æ•°æ®æ¨¡å‹ ====================

@dataclass
class EvaluationRequest:
    """è¯„ä¼°è¯·æ±‚"""
    user_id: int
    test_id: int
    answers: List[Dict[str, Any]]
    strategy: EvaluationStrategy
    preferred_methods: List[EvaluationMethod]
    api_sources: List[APISource]
    include_flower_analysis: bool = True
    include_career_analysis: bool = True
    timeout: int = 30
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class EvaluationResult:
    """è¯„ä¼°ç»“æœ"""
    mbti_type: str
    confidence_level: float
    evaluation_method: str
    api_source: Optional[str]
    processing_time: float
    result_data: Dict[str, Any]
    flower_analysis: Optional[Dict[str, Any]] = None
    career_analysis: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class APIConfig:
    """APIé…ç½®"""
    source: APISource
    endpoint: str
    api_key: str
    api_secret: Optional[str] = None
    rate_limit: int = 100
    timeout: int = 30
    cost_per_request: float = 0.0
    is_active: bool = True
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class EvaluationMetrics:
    """è¯„ä¼°æŒ‡æ ‡"""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    average_response_time: float = 0.0
    api_usage_count: Dict[str, int] = None
    local_usage_count: int = 0
    hybrid_usage_count: int = 0
    
    def __post_init__(self):
        if self.api_usage_count is None:
            self.api_usage_count = {}
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


# ==================== æœ¬åœ°è¯„ä¼°å¼•æ“ ====================

class LocalMBTIAssessmentEngine:
    """æœ¬åœ°MBTIè¯„ä¼°å¼•æ“"""
    
    def __init__(self):
        self.question_weights = self._initialize_question_weights()
        self.dimension_mappings = self._initialize_dimension_mappings()
        self.confidence_threshold = 0.6
    
    def _initialize_question_weights(self) -> Dict[str, float]:
        """åˆå§‹åŒ–é¢˜ç›®æƒé‡"""
        return {
            "EI": {"E": 1.0, "I": -1.0},
            "SN": {"S": 1.0, "N": -1.0},
            "TF": {"T": 1.0, "F": -1.0},
            "JP": {"J": 1.0, "P": -1.0}
        }
    
    def _initialize_dimension_mappings(self) -> Dict[str, str]:
        """åˆå§‹åŒ–ç»´åº¦æ˜ å°„"""
        return {
            "EI": "å¤–å‘/å†…å‘",
            "SN": "æ„Ÿè§‰/ç›´è§‰", 
            "TF": "æ€è€ƒ/æƒ…æ„Ÿ",
            "JP": "åˆ¤æ–­/æ„ŸçŸ¥"
        }
    
    async def evaluate(self, answers: List[Dict[str, Any]]) -> EvaluationResult:
        """æ‰§è¡Œæœ¬åœ°è¯„ä¼°"""
        start_time = datetime.now()
        
        try:
            # è®¡ç®—å„ç»´åº¦åˆ†æ•°
            dimension_scores = self._calculate_dimension_scores(answers)
            
            # ç¡®å®šMBTIç±»å‹
            mbti_type = self._determine_mbti_type(dimension_scores)
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_confidence(dimension_scores)
            
            # ç”Ÿæˆç»“æœæ•°æ®
            result_data = {
                "dimension_scores": dimension_scores,
                "mbti_type": mbti_type,
                "confidence_level": confidence,
                "assessment_details": self._generate_assessment_details(dimension_scores)
            }
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return EvaluationResult(
                mbti_type=mbti_type,
                confidence_level=confidence,
                evaluation_method="local_mbti_standard",
                api_source=None,
                processing_time=processing_time,
                result_data=result_data
            )
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            return EvaluationResult(
                mbti_type="",
                confidence_level=0.0,
                evaluation_method="local_mbti_standard",
                api_source=None,
                processing_time=processing_time,
                result_data={},
                error_message=str(e)
            )
    
    def _calculate_dimension_scores(self, answers: List[Dict[str, Any]]) -> Dict[str, float]:
        """è®¡ç®—ç»´åº¦åˆ†æ•°"""
        dimension_scores = {"EI": 0, "SN": 0, "TF": 0, "JP": 0}
        
        for answer in answers:
            question_id = answer.get("question_id")
            answer_value = answer.get("answer_value")
            dimension = answer.get("dimension")
            
            if dimension in dimension_scores:
                # æ ¹æ®ç­”æ¡ˆå€¼è®¡ç®—åˆ†æ•°
                score = self._get_answer_score(answer_value, dimension)
                dimension_scores[dimension] += score
        
        return dimension_scores
    
    def _get_answer_score(self, answer_value: str, dimension: str) -> float:
        """è·å–ç­”æ¡ˆåˆ†æ•°"""
        # ç®€åŒ–çš„è¯„åˆ†é€»è¾‘ï¼Œå®é™…åº”è¯¥æ ¹æ®é¢˜ç›®å’Œç­”æ¡ˆçš„è¯¦ç»†é…ç½®
        score_mapping = {
            "A": 1.0, "B": 0.5, "C": 0.0, "D": -0.5, "E": -1.0
        }
        
        base_score = score_mapping.get(answer_value, 0.0)
        
        # æ ¹æ®ç»´åº¦è°ƒæ•´æƒé‡
        dimension_weights = self.question_weights.get(dimension, {})
        return base_score * 1.0  # ç®€åŒ–å¤„ç†
    
    def _determine_mbti_type(self, dimension_scores: Dict[str, float]) -> str:
        """ç¡®å®šMBTIç±»å‹"""
        mbti_type = ""
        
        # EIç»´åº¦
        mbti_type += "E" if dimension_scores["EI"] > 0 else "I"
        
        # SNç»´åº¦
        mbti_type += "S" if dimension_scores["SN"] > 0 else "N"
        
        # TFç»´åº¦
        mbti_type += "T" if dimension_scores["TF"] > 0 else "F"
        
        # JPç»´åº¦
        mbti_type += "J" if dimension_scores["JP"] > 0 else "P"
        
        return mbti_type
    
    def _calculate_confidence(self, dimension_scores: Dict[str, float]) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦"""
        total_confidence = 0.0
        
        for dimension, score in dimension_scores.items():
            # è®¡ç®—æ¯ä¸ªç»´åº¦çš„ç½®ä¿¡åº¦
            confidence = abs(score) / 10.0  # å‡è®¾æœ€å¤§åˆ†æ•°ä¸º10
            total_confidence += confidence
        
        return min(total_confidence / 4.0, 1.0)  # 4ä¸ªç»´åº¦çš„å¹³å‡ç½®ä¿¡åº¦


# ==================== AIè‡ªé€‚åº”æµ‹è¯•å¼•æ“ ====================

class AIAdaptiveTestEngine:
    """AIè‡ªé€‚åº”æµ‹è¯•å¼•æ“ - åŸºäºå¥¥æ€MBTIçš„æ™ºèƒ½è‡ªé€‚åº”è®¾è®¡"""
    
    def __init__(self):
        self.question_weights = {}
        self.user_response_patterns = {}
        self.adaptive_algorithm = AdaptiveAlgorithm()
        
    def analyze_user_behavior(self, user_responses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼ - åŸºäºå¥¥æ€MBTIçš„æ·±åº¦å­¦ä¹ æ¨¡å‹"""
        # åˆ†æä½œç­”æ¨¡å¼
        response_patterns = self._analyze_response_patterns(user_responses)
        
        # åˆ†ææ—¶é—´å› ç´ 
        timing_analysis = self._analyze_timing_patterns(user_responses)
        
        # åˆ†æä¸€è‡´æ€§
        consistency_score = self._calculate_consistency_score(user_responses)
        
        return {
            "response_patterns": response_patterns,
            "timing_analysis": timing_analysis,
            "consistency_score": consistency_score,
            "confidence_level": self._calculate_ai_confidence(response_patterns, timing_analysis, consistency_score)
        }
    
    def _analyze_response_patterns(self, responses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æç”¨æˆ·ä½œç­”æ¨¡å¼"""
        patterns = {
            "extreme_responses": 0,  # æç«¯å›ç­”æ¯”ä¾‹
            "neutral_responses": 0,  # ä¸­æ€§å›ç­”æ¯”ä¾‹
            "response_consistency": 0,  # å›ç­”ä¸€è‡´æ€§
            "dimension_preference": {}  # ç»´åº¦åå¥½
        }
        
        for response in responses:
            if response.get("answer_value") in ["A", "E"]:  # æç«¯é€‰é¡¹
                patterns["extreme_responses"] += 1
            elif response.get("answer_value") == "C":  # ä¸­æ€§é€‰é¡¹
                patterns["neutral_responses"] += 1
        
        patterns["extreme_responses"] = patterns["extreme_responses"] / len(responses)
        patterns["neutral_responses"] = patterns["neutral_responses"] / len(responses)
        
        return patterns
    
    def _analyze_timing_patterns(self, responses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææ—¶é—´æ¨¡å¼"""
        if not responses:
            return {"average_time": 0, "time_consistency": 0}
        
        times = [r.get("response_time", 0) for r in responses if r.get("response_time")]
        if not times:
            return {"average_time": 0, "time_consistency": 0}
        
        avg_time = sum(times) / len(times)
        time_variance = sum((t - avg_time) ** 2 for t in times) / len(times)
        time_consistency = 1 / (1 + time_variance)  # æ—¶é—´ä¸€è‡´æ€§å¾—åˆ†
        
        return {
            "average_time": avg_time,
            "time_consistency": time_consistency,
            "time_pattern": "fast" if avg_time < 10 else "slow" if avg_time > 30 else "normal"
        }
    
    def _calculate_consistency_score(self, responses: List[Dict[str, Any]]) -> float:
        """è®¡ç®—å›ç­”ä¸€è‡´æ€§å¾—åˆ†"""
        if len(responses) < 2:
            return 1.0
        
        # åˆ†æåŒä¸€ç»´åº¦å†…å›ç­”çš„ä¸€è‡´æ€§
        dimension_responses = {}
        for response in responses:
            dimension = response.get("dimension")
            if dimension not in dimension_responses:
                dimension_responses[dimension] = []
            dimension_responses[dimension].append(response.get("answer_value"))
        
        consistency_scores = []
        for dimension, answers in dimension_responses.items():
            if len(answers) > 1:
                # è®¡ç®—è¯¥ç»´åº¦å†…å›ç­”çš„ä¸€è‡´æ€§
                consistency = len(set(answers)) / len(answers)
                consistency_scores.append(consistency)
        
        return sum(consistency_scores) / len(consistency_scores) if consistency_scores else 1.0
    
    def _calculate_ai_confidence(self, patterns: Dict, timing: Dict, consistency: float) -> float:
        """è®¡ç®—AIç½®ä¿¡åº¦ - åŸºäºå¥¥æ€MBTIçš„96.8%å‡†ç¡®ç‡è®¾è®¡"""
        # åŸºäºå¤šä¸ªå› ç´ è®¡ç®—ç½®ä¿¡åº¦
        pattern_confidence = 1 - abs(patterns["extreme_responses"] - 0.5)  # æç«¯å›ç­”é€‚ä¸­
        timing_confidence = timing["time_consistency"]
        consistency_confidence = consistency
        
        # åŠ æƒå¹³å‡
        ai_confidence = (pattern_confidence * 0.4 + timing_confidence * 0.3 + consistency_confidence * 0.3)
        
        # åŸºäºå¥¥æ€MBTIçš„å‡†ç¡®ç‡è°ƒæ•´
        return min(ai_confidence * 0.968, 1.0)  # 96.8%å‡†ç¡®ç‡è°ƒæ•´


class UserBehaviorAnalyzer:
    """ç”¨æˆ·è¡Œä¸ºåˆ†æå™¨ - åŸºäºå¥¥æ€MBTIçš„è¡Œä¸ºè§£æè®¾è®¡"""
    
    def __init__(self):
        self.behavior_patterns = {}
        self.learning_model = None  # æ·±åº¦å­¦ä¹ æ¨¡å‹
    
    def analyze_behavioral_patterns(self, user_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼"""
        return {
            "response_style": self._classify_response_style(user_data),
            "thinking_pattern": self._analyze_thinking_pattern(user_data),
            "decision_making": self._analyze_decision_making(user_data),
            "personality_indicators": self._extract_personality_indicators(user_data)
        }
    
    def _classify_response_style(self, user_data: Dict[str, Any]) -> str:
        """åˆ†ç±»ç”¨æˆ·å›ç­”é£æ ¼"""
        # åŸºäºå¥¥æ€MBTIçš„AIåˆ†æ
        if user_data.get("average_response_time", 0) < 10:
            return "intuitive"  # ç›´è§‰å‹
        elif user_data.get("average_response_time", 0) > 30:
            return "analytical"  # åˆ†æå‹
        else:
            return "balanced"  # å¹³è¡¡å‹
    
    def _analyze_thinking_pattern(self, user_data: Dict[str, Any]) -> str:
        """åˆ†ææ€ç»´æ¨¡å¼"""
        # åŸºäºå›ç­”æ¨¡å¼åˆ†ææ€ç»´ç±»å‹
        if user_data.get("extreme_responses", 0) > 0.7:
            return "decisive"  # å†³æ–­å‹
        elif user_data.get("neutral_responses", 0) > 0.5:
            return "flexible"  # çµæ´»å‹
        else:
            return "balanced"  # å¹³è¡¡å‹
    
    def _analyze_decision_making(self, user_data: Dict[str, Any]) -> str:
        """åˆ†æå†³ç­–æ¨¡å¼"""
        # åŸºäºä¸€è‡´æ€§åˆ†æå†³ç­–æ¨¡å¼
        consistency = user_data.get("consistency_score", 0)
        if consistency > 0.8:
            return "structured"  # ç»“æ„åŒ–
        elif consistency < 0.5:
            return "adaptive"  # é€‚åº”æ€§
        else:
            return "mixed"  # æ··åˆå‹
    
    def _extract_personality_indicators(self, user_data: Dict[str, Any]) -> List[str]:
        """æå–äººæ ¼æŒ‡æ ‡"""
        indicators = []
        
        if user_data.get("response_style") == "intuitive":
            indicators.append("N")  # ç›´è§‰
        else:
            indicators.append("S")  # æ„Ÿè§‰
        
        if user_data.get("thinking_pattern") == "decisive":
            indicators.append("T")  # æ€è€ƒ
        else:
            indicators.append("F")  # æƒ…æ„Ÿ
        
        return indicators


class DynamicQuestionSelector:
    """åŠ¨æ€é¢˜ç›®é€‰æ‹©å™¨ - åŸºäºå¥¥æ€MBTIçš„è‡ªé€‚åº”é€‰é¢˜"""
    
    def __init__(self):
        self.question_bank = {}
        self.adaptive_rules = {}
        
    def select_next_questions(self, current_responses: List[Dict], remaining_questions: int) -> List[int]:
        """åŠ¨æ€é€‰æ‹©ä¸‹ä¸€æ‰¹é¢˜ç›®"""
        # åŸºäºå½“å‰å›ç­”åˆ†æé€‰æ‹©æœ€ç›¸å…³çš„é¢˜ç›®
        analysis = self._analyze_current_responses(current_responses)
        selected_questions = self._apply_adaptive_rules(analysis, remaining_questions)
        
        return selected_questions
    
    def _analyze_current_responses(self, responses: List[Dict]) -> Dict[str, Any]:
        """åˆ†æå½“å‰å›ç­”"""
        return {
            "dimension_coverage": self._calculate_dimension_coverage(responses),
            "confidence_levels": self._calculate_confidence_levels(responses),
            "unclear_dimensions": self._identify_unclear_dimensions(responses)
        }
    
    def _calculate_dimension_coverage(self, responses: List[Dict]) -> Dict[str, float]:
        """è®¡ç®—ç»´åº¦è¦†ç›–åº¦"""
        coverage = {"EI": 0, "SN": 0, "TF": 0, "JP": 0}
        
        for response in responses:
            dimension = response.get("dimension")
            if dimension in coverage:
                coverage[dimension] += 1
        
        # è½¬æ¢ä¸ºæ¯”ä¾‹
        total = sum(coverage.values())
        if total > 0:
            for dim in coverage:
                coverage[dim] = coverage[dim] / total
        
        return coverage
    
    def _calculate_confidence_levels(self, responses: List[Dict]) -> Dict[str, float]:
        """è®¡ç®—å„ç»´åº¦ç½®ä¿¡åº¦"""
        confidence = {"EI": 0, "SN": 0, "TF": 0, "JP": 0}
        
        # åŸºäºå›ç­”ä¸€è‡´æ€§è®¡ç®—ç½®ä¿¡åº¦
        for response in responses:
            dimension = response.get("dimension")
            if dimension in confidence:
                # ç®€åŒ–çš„ç½®ä¿¡åº¦è®¡ç®—
                confidence[dimension] += 0.1
        
        return confidence
    
    def _identify_unclear_dimensions(self, responses: List[Dict]) -> List[str]:
        """è¯†åˆ«ä¸æ¸…æ™°çš„ç»´åº¦"""
        unclear = []
        coverage = self._calculate_dimension_coverage(responses)
        confidence = self._calculate_confidence_levels(responses)
        
        for dimension in ["EI", "SN", "TF", "JP"]:
            if coverage.get(dimension, 0) < 0.2 or confidence.get(dimension, 0) < 0.5:
                unclear.append(dimension)
        
        return unclear
    
    def _apply_adaptive_rules(self, analysis: Dict, remaining_questions: int) -> List[int]:
        """åº”ç”¨è‡ªé€‚åº”è§„åˆ™é€‰æ‹©é¢˜ç›®"""
        # ä¼˜å…ˆé€‰æ‹©ä¸æ¸…æ™°ç»´åº¦çš„é¢˜ç›®
        unclear_dimensions = analysis.get("unclear_dimensions", [])
        
        # åŸºäºå¥¥æ€MBTIçš„æ™ºèƒ½é€‰é¢˜é€»è¾‘
        selected_questions = []
        
        # ä¸ºæ¯ä¸ªä¸æ¸…æ™°çš„ç»´åº¦é€‰æ‹©é¢˜ç›®
        questions_per_dimension = remaining_questions // max(len(unclear_dimensions), 1)
        
        for dimension in unclear_dimensions:
            # é€‰æ‹©è¯¥ç»´åº¦çš„é¢˜ç›®ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
            for i in range(questions_per_dimension):
                selected_questions.append(i)  # å®é™…åº”è¯¥ä»é¢˜åº“ä¸­é€‰æ‹©
        
        return selected_questions


class AdaptiveAlgorithm:
    """è‡ªé€‚åº”ç®—æ³• - åŸºäºå¥¥æ€MBTIçš„æœºå™¨å­¦ä¹ ç®—æ³•"""
    
    def __init__(self):
        self.learning_rate = 0.01
        self.model_weights = {}
        
    def update_model(self, user_feedback: Dict[str, Any]):
        """æ›´æ–°æ¨¡å‹ - åŸºäºç”¨æˆ·åé¦ˆæŒç»­ä¼˜åŒ–"""
        # åŸºäºå¥¥æ€MBTIçš„æŒç»­ä¼˜åŒ–æœºåˆ¶
        pass
    
    def predict_optimal_questions(self, user_profile: Dict[str, Any]) -> List[int]:
        """é¢„æµ‹æœ€ä¼˜é¢˜ç›®åºåˆ—"""
        # åŸºäºç”¨æˆ·ç”»åƒé¢„æµ‹æœ€ä¼˜é¢˜ç›®
        pass
    
    def validate_user_input(self, user_input: str) -> Dict[str, Any]:
        """éªŒè¯ç”¨æˆ·è¾“å…¥ - åŸºäºå¾®åšç”¨æˆ·MBTIç±»å‹è¯†åˆ«æŠ€æœ¯"""
        # åŸºäºå­¦ä¹ æˆæœçš„æ­£åˆ™è¡¨è¾¾å¼éªŒè¯
        mbti_pattern = r'(?:infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj)'
        matches = re.findall(mbti_pattern, user_input.lower())
        
        if matches:
            # æå–ä¸Šä¸‹æ–‡
            context_pattern = r"(.{0,10}(?:infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj).{0,10})"
            contexts = re.findall(context_pattern, user_input.lower())
            
            return {
                "is_valid": True,
                "mbti_types": list(set(matches)),
                "contexts": contexts,
                "confidence": min(len(matches) / 3.0, 1.0),
                "user_language_style": self._classify_user_language_style(user_input)
            }
        
        return {
            "is_valid": False,
            "mbti_types": [],
            "contexts": [],
            "confidence": 0.0,
            "user_language_style": "unknown"
        }
    
    def _classify_user_language_style(self, text: str) -> str:
        """åˆ†ç±»ç”¨æˆ·è¯­è¨€é£æ ¼"""
        if len(text) > 100:
            return "detailed"
        elif "ï¼" in text or "ï¼Ÿ" in text:
            return "expressive"
        elif "ã€‚" in text and len(text.split("ã€‚")) > 3:
            return "analytical"
        else:
            return "concise"
    
    def _generate_assessment_details(self, dimension_scores: Dict[str, float]) -> Dict[str, Any]:
        """ç”Ÿæˆè¯„ä¼°è¯¦æƒ…"""
        details = {}
        
        for dimension, score in dimension_scores.items():
            details[dimension] = {
                "score": score,
                "description": self.dimension_mappings.get(dimension, ""),
                "strength": "å¼º" if abs(score) > 5 else "ä¸­ç­‰" if abs(score) > 2 else "å¼±"
            }
        
        return details


# ==================== å¤–éƒ¨APIè¯„ä¼°å¼•æ“ ====================

class ExternalAPIAssessmentEngine:
    """å¤–éƒ¨APIè¯„ä¼°å¼•æ“"""
    
    def __init__(self):
        self.api_configs = self._initialize_api_configs()
        self.session = None
    
    def _initialize_api_configs(self) -> Dict[APISource, APIConfig]:
        """åˆå§‹åŒ–APIé…ç½®"""
        configs = {}
        
        # æé€Ÿæ•°æ®APIé…ç½®
        configs[APISource.JISU_DATA] = APIConfig(
            source=APISource.JISU_DATA,
            endpoint="https://api.jisuapi.com/mbti/analyze",
            api_key="your_jisu_api_key",
            rate_limit=100,
            timeout=30,
            cost_per_request=0.01
        )
        
        # æŒ–æ•°æ®APIé…ç½®
        configs[APISource.WADATA] = APIConfig(
            source=APISource.WADATA,
            endpoint="https://api.wadata.com/mbti/analyze",
            api_key="your_wadata_api_key",
            rate_limit=50,
            timeout=30,
            cost_per_request=0.02
        )
        
        # é˜¿é‡Œäº‘APIé…ç½®
        configs[APISource.ALIYUN] = APIConfig(
            source=APISource.ALIYUN,
            endpoint="https://api.aliyun.com/mbti/analyze",
            api_key="your_aliyun_api_key",
            rate_limit=200,
            timeout=30,
            cost_per_request=0.005
        )
        
        return configs
    
    async def evaluate(self, answers: List[Dict[str, Any]], api_source: APISource) -> EvaluationResult:
        """æ‰§è¡Œå¤–éƒ¨APIè¯„ä¼°"""
        start_time = datetime.now()
        
        if api_source not in self.api_configs:
            return EvaluationResult(
                mbti_type="",
                confidence_level=0.0,
                evaluation_method="external_api",
                api_source=api_source.value,
                processing_time=0.0,
                result_data={},
                error_message=f"ä¸æ”¯æŒçš„APIæ¥æº: {api_source}"
            )
        
        config = self.api_configs[api_source]
        
        if not config.is_active:
            return EvaluationResult(
                mbti_type="",
                confidence_level=0.0,
                evaluation_method="external_api",
                api_source=api_source.value,
                processing_time=0.0,
                result_data={},
                error_message=f"APIæ¥æº {api_source} æœªæ¿€æ´»"
            )
        
        try:
            # å‡†å¤‡APIè¯·æ±‚æ•°æ®
            request_data = self._prepare_api_request(answers, api_source)
            
            # å‘é€APIè¯·æ±‚
            response_data = await self._send_api_request(config, request_data)
            
            # è§£æAPIå“åº”
            mbti_type, confidence, result_data = self._parse_api_response(response_data, api_source)
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return EvaluationResult(
                mbti_type=mbti_type,
                confidence_level=confidence,
                evaluation_method="external_api",
                api_source=api_source.value,
                processing_time=processing_time,
                result_data=result_data
            )
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            return EvaluationResult(
                mbti_type="",
                confidence_level=0.0,
                evaluation_method="external_api",
                api_source=api_source.value,
                processing_time=processing_time,
                result_data={},
                error_message=str(e)
            )
    
    def _prepare_api_request(self, answers: List[Dict[str, Any]], api_source: APISource) -> Dict[str, Any]:
        """å‡†å¤‡APIè¯·æ±‚æ•°æ®"""
        if api_source == APISource.JISU_DATA:
            return {
                "answers": answers,
                "test_type": "standard",
                "include_analysis": True
            }
        elif api_source == APISource.WADATA:
            return {
                "question_answers": answers,
                "analysis_type": "comprehensive",
                "include_career": True
            }
        elif api_source == APISource.ALIYUN:
            return {
                "mbti_answers": answers,
                "analysis_level": "detailed",
                "include_insights": True
            }
        else:
            return {"answers": answers}
    
    async def _send_api_request(self, config: APIConfig, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """å‘é€APIè¯·æ±‚"""
        if not self.session:
            self.session = aiohttp.ClientSession()
        
        headers = {
            "Authorization": f"Bearer {config.api_key}",
            "Content-Type": "application/json"
        }
        
        async with self.session.post(
            config.endpoint,
            json=request_data,
            headers=headers,
            timeout=aiohttp.ClientTimeout(total=config.timeout)
        ) as response:
            if response.status == 200:
                return await response.json()
            else:
                error_text = await response.text()
                raise Exception(f"APIè¯·æ±‚å¤±è´¥: {response.status} - {error_text}")
    
    def _parse_api_response(self, response_data: Dict[str, Any], api_source: APISource) -> Tuple[str, float, Dict[str, Any]]:
        """è§£æAPIå“åº”"""
        if api_source == APISource.JISU_DATA:
            return (
                response_data.get("mbti_type", ""),
                response_data.get("confidence", 0.0),
                response_data.get("analysis", {})
            )
        elif api_source == APISource.WADATA:
            return (
                response_data.get("personality_type", ""),
                response_data.get("confidence_level", 0.0),
                response_data.get("detailed_analysis", {})
            )
        elif api_source == APISource.ALIYUN:
            return (
                response_data.get("type", ""),
                response_data.get("confidence", 0.0),
                response_data.get("insights", {})
            )
        else:
            return ("", 0.0, {})


# ==================== æ··åˆè¯„ä¼°ç­–ç•¥å¼•æ“ ====================

class HybridEvaluationEngine:
    """æ··åˆè¯„ä¼°ç­–ç•¥å¼•æ“ - åŸºäºAIé©±åŠ¨çš„æ™ºèƒ½è‡ªé€‚åº”æµ‹è¯•"""
    
    def __init__(self):
        self.local_engine = LocalMBTIAssessmentEngine()
        self.api_engine = ExternalAPIAssessmentEngine()
        self.metrics = EvaluationMetrics()
        self.logger = logging.getLogger(__name__)
        
        # åŸºäºå¥¥æ€MBTIçš„AIé©±åŠ¨è®¾è®¡
        self.ai_adaptive_engine = AIAdaptiveTestEngine()
        self.user_behavior_analyzer = UserBehaviorAnalyzer()
        self.dynamic_question_selector = DynamicQuestionSelector()
    
    async def evaluate(self, request: EvaluationRequest) -> EvaluationResult:
        """æ‰§è¡Œæ··åˆè¯„ä¼°"""
        self.metrics.total_requests += 1
        
        try:
            if request.strategy == EvaluationStrategy.LOCAL_ONLY:
                return await self._evaluate_local_only(request)
            elif request.strategy == EvaluationStrategy.API_ONLY:
                return await self._evaluate_api_only(request)
            elif request.strategy == EvaluationStrategy.HYBRID:
                return await self._evaluate_hybrid(request)
            elif request.strategy == EvaluationStrategy.FALLBACK:
                return await self._evaluate_fallback(request)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„è¯„ä¼°ç­–ç•¥: {request.strategy}")
                
        except Exception as e:
            self.metrics.failed_requests += 1
            self.logger.error(f"è¯„ä¼°å¤±è´¥: {str(e)}")
            return EvaluationResult(
                mbti_type="",
                confidence_level=0.0,
                evaluation_method="error",
                api_source=None,
                processing_time=0.0,
                result_data={},
                error_message=str(e)
            )
    
    async def _evaluate_local_only(self, request: EvaluationRequest) -> EvaluationResult:
        """ä»…æœ¬åœ°è¯„ä¼°"""
        result = await self.local_engine.evaluate(request.answers)
        self.metrics.local_usage_count += 1
        self.metrics.successful_requests += 1
        return result
    
    async def _evaluate_api_only(self, request: EvaluationRequest) -> EvaluationResult:
        """ä»…APIè¯„ä¼°"""
        best_result = None
        best_confidence = 0.0
        
        for api_source in request.api_sources:
            try:
                result = await self.api_engine.evaluate(request.answers, api_source)
                if result.confidence_level > best_confidence:
                    best_result = result
                    best_confidence = result.confidence_level
                
                # è®°å½•APIä½¿ç”¨
                if api_source.value not in self.metrics.api_usage_count:
                    self.metrics.api_usage_count[api_source.value] = 0
                self.metrics.api_usage_count[api_source.value] += 1
                
            except Exception as e:
                self.logger.warning(f"API {api_source} è¯„ä¼°å¤±è´¥: {str(e)}")
                continue
        
        if best_result:
            self.metrics.successful_requests += 1
            return best_result
        else:
            self.metrics.failed_requests += 1
            return EvaluationResult(
                mbti_type="",
                confidence_level=0.0,
                evaluation_method="api_only",
                api_source=None,
                processing_time=0.0,
                result_data={},
                error_message="æ‰€æœ‰APIè¯„ä¼°å¤±è´¥"
            )
    
    async def _evaluate_hybrid(self, request: EvaluationRequest) -> EvaluationResult:
        """æ··åˆè¯„ä¼°"""
        # å¹¶è¡Œæ‰§è¡Œæœ¬åœ°å’ŒAPIè¯„ä¼°
        local_task = asyncio.create_task(self.local_engine.evaluate(request.answers))
        api_tasks = []
        
        for api_source in request.api_sources:
            api_tasks.append(
                asyncio.create_task(self.api_engine.evaluate(request.answers, api_source))
            )
        
        # ç­‰å¾…æ‰€æœ‰è¯„ä¼°å®Œæˆ
        local_result = await local_task
        api_results = await asyncio.gather(*api_tasks, return_exceptions=True)
        
        # é€‰æ‹©æœ€ä½³ç»“æœ
        best_result = local_result
        best_confidence = local_result.confidence_level
        
        for api_result in api_results:
            if isinstance(api_result, EvaluationResult) and api_result.confidence_level > best_confidence:
                best_result = api_result
                best_confidence = api_result.confidence_level
        
        # è®°å½•ä½¿ç”¨ç»Ÿè®¡
        self.metrics.hybrid_usage_count += 1
        self.metrics.local_usage_count += 1
        
        for api_source in request.api_sources:
            if api_source.value not in self.metrics.api_usage_count:
                self.metrics.api_usage_count[api_source.value] = 0
            self.metrics.api_usage_count[api_source.value] += 1
        
        self.metrics.successful_requests += 1
        return best_result
    
    async def _evaluate_fallback(self, request: EvaluationRequest) -> EvaluationResult:
        """é™çº§è¯„ä¼°"""
        # å…ˆå°è¯•APIè¯„ä¼°
        for api_source in request.api_sources:
            try:
                result = await self.api_engine.evaluate(request.answers, api_source)
                if result.mbti_type and result.confidence_level > 0.5:
                    self.metrics.successful_requests += 1
                    return result
            except Exception as e:
                self.logger.warning(f"API {api_source} é™çº§è¯„ä¼°å¤±è´¥: {str(e)}")
                continue
        
        # APIå¤±è´¥ï¼Œé™çº§åˆ°æœ¬åœ°è¯„ä¼°
        local_result = await self.local_engine.evaluate(request.answers)
        self.metrics.local_usage_count += 1
        self.metrics.successful_requests += 1
        return local_result
    
    def get_metrics(self) -> EvaluationMetrics:
        """è·å–è¯„ä¼°æŒ‡æ ‡"""
        return self.metrics
    
    def reset_metrics(self):
        """é‡ç½®è¯„ä¼°æŒ‡æ ‡"""
        self.metrics = EvaluationMetrics()


# ==================== ä¸»å‡½æ•°å’Œç¤ºä¾‹ ====================

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”„ MBTIæ··åˆè¯„ä¼°ç­–ç•¥è®¾è®¡")
    print("ç‰ˆæœ¬: v1.5 (åä¸­å¸ˆèŒƒå¤§å­¦åˆ›æ–°ç‰ˆ)")
    print("åŸºäº: æœ¬åœ°æ ¸å¿ƒ+å¤–éƒ¨å¢å¼ºçš„æ··åˆè¯„ä¼°ç­–ç•¥")
    print("=" * 60)
    
    # åˆå§‹åŒ–æ··åˆè¯„ä¼°å¼•æ“
    hybrid_engine = HybridEvaluationEngine()
    
    # ç¤ºä¾‹è¯„ä¼°è¯·æ±‚
    sample_answers = [
        {"question_id": 1, "answer_value": "A", "dimension": "EI"},
        {"question_id": 2, "answer_value": "B", "dimension": "SN"},
        {"question_id": 3, "answer_value": "C", "dimension": "TF"},
        {"question_id": 4, "answer_value": "D", "dimension": "JP"}
    ]
    
    # æµ‹è¯•ä¸åŒè¯„ä¼°ç­–ç•¥
    strategies = [
        EvaluationStrategy.LOCAL_ONLY,
        EvaluationStrategy.HYBRID,
        EvaluationStrategy.FALLBACK
    ]
    
    for strategy in strategies:
        print(f"\nğŸ“Š æµ‹è¯•è¯„ä¼°ç­–ç•¥: {strategy.value}")
        
        request = EvaluationRequest(
            user_id=1,
            test_id=1,
            answers=sample_answers,
            strategy=strategy,
            preferred_methods=[EvaluationMethod.MBTI_STANDARD],
            api_sources=[APISource.JISU_DATA, APISource.ALIYUN]
        )
        
        result = await hybrid_engine.evaluate(request)
        print(f"âœ… è¯„ä¼°ç»“æœ: {result.mbti_type}")
        print(f"   ç½®ä¿¡åº¦: {result.confidence_level:.2f}")
        print(f"   è¯„ä¼°æ–¹æ³•: {result.evaluation_method}")
        print(f"   APIæ¥æº: {result.api_source}")
        print(f"   å¤„ç†æ—¶é—´: {result.processing_time:.2f}ç§’")
    
    # æ˜¾ç¤ºè¯„ä¼°æŒ‡æ ‡
    metrics = hybrid_engine.get_metrics()
    print(f"\nğŸ“ˆ è¯„ä¼°æŒ‡æ ‡:")
    print(f"   æ€»è¯·æ±‚æ•°: {metrics.total_requests}")
    print(f"   æˆåŠŸè¯·æ±‚æ•°: {metrics.successful_requests}")
    print(f"   å¤±è´¥è¯·æ±‚æ•°: {metrics.failed_requests}")
    print(f"   æœ¬åœ°ä½¿ç”¨æ¬¡æ•°: {metrics.local_usage_count}")
    print(f"   æ··åˆä½¿ç”¨æ¬¡æ•°: {metrics.hybrid_usage_count}")
    
    print("\nğŸ‰ æ··åˆè¯„ä¼°ç­–ç•¥è®¾è®¡å®Œæˆï¼")
    print("ğŸ“‹ æ”¯æŒçš„åŠŸèƒ½:")
    print("  - æœ¬åœ°æ ¸å¿ƒè¯„ä¼°")
    print("  - å¤–éƒ¨APIå¢å¼º")
    print("  - æ··åˆè¯„ä¼°ç­–ç•¥")
    print("  - é™çº§è¯„ä¼°æœºåˆ¶")
    print("  - è¯„ä¼°æŒ‡æ ‡ç›‘æ§")


if __name__ == "__main__":
    asyncio.run(main())
