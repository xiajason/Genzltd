# AIå¤§æ¨¡å‹éƒ¨ç½²å®æ–½æ–¹æ¡ˆ

## ğŸ¯ å›ç­”æ‚¨çš„é—®é¢˜ï¼šæ¨¡å‹ä¸‹è½½ç­–ç•¥

### æ˜¯çš„ï¼Œè¿™äº›æ¨¡å‹éƒ½éœ€è¦ä¸‹è½½åˆ°æœ¬åœ°ï¼

**åŸå› **ï¼š
1. **æ€§èƒ½è¦æ±‚**: æœ¬åœ°æ¨¡å‹å“åº”æ—¶é—´<100msï¼Œåœ¨çº¿APIé€šå¸¸>1s
2. **ç¨³å®šæ€§**: é¿å…ç½‘ç»œæ³¢åŠ¨å½±å“æœåŠ¡å¯ç”¨æ€§
3. **æˆæœ¬æ§åˆ¶**: é¿å…é¢‘ç¹è°ƒç”¨å¤–éƒ¨APIäº§ç”Ÿè´¹ç”¨
4. **æ•°æ®å®‰å…¨**: æ•æ„Ÿæ•°æ®ä¸ç¦»å¼€æœ¬åœ°ç¯å¢ƒ

## ğŸ“Š å½“å‰æ¨¡å‹ä¸‹è½½æƒ…å†µ

### å·²ä¸‹è½½æ¨¡å‹
- **æ¨¡å‹**: `sentence-transformers/all-MiniLM-L6-v2`
- **å¤§å°**: 909MB
- **ä½ç½®**: `/home/appuser/.cache/torch/sentence_transformers/`
- **çŠ¶æ€**: âœ… å·²å¯ç”¨

### ä¸‹è½½æ–¹å¼åˆ†æ
```bash
# å½“å‰æ–¹å¼ï¼šå®¹å™¨å†…åŠ¨æ€ä¸‹è½½
# ä¼˜ç‚¹ï¼šæŒ‰éœ€ä¸‹è½½ï¼ŒèŠ‚çœç©ºé—´
# ç¼ºç‚¹ï¼šé¦–æ¬¡ä½¿ç”¨æ…¢ï¼Œç½‘ç»œä¾èµ–
```

## ğŸš€ æ¨èéƒ¨ç½²æ–¹æ¡ˆ

### æ–¹æ¡ˆä¸€ï¼šé¢„ä¸‹è½½åˆ°å®¹å™¨é•œåƒ (æ¨è)

#### 1. ä¿®æ”¹Dockerfile
```dockerfile
# åœ¨AIæœåŠ¡Dockerfileä¸­æ·»åŠ 
FROM python:3.11-slim

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install -r requirements.txt

# é¢„ä¸‹è½½æ ¸å¿ƒæ¨¡å‹
RUN python -c "
from sentence_transformers import SentenceTransformer
models = [
    'sentence-transformers/all-MiniLM-L6-v2',      # å¿«é€Ÿæ¨¡å‹
    'sentence-transformers/all-MiniLM-L12-v2',     # å¹³è¡¡æ¨¡å‹  
    'sentence-transformers/all-mpnet-base-v2'      # é«˜ç²¾åº¦æ¨¡å‹
]
for model in models:
    print(f'Downloading {model}...')
    SentenceTransformer(model)
    print(f'{model} downloaded successfully')
"

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . /app
WORKDIR /app
```

#### 2. æ›´æ–°docker-compose.yml
```yaml
services:
  ai-models:
    build: ./ai-services/ai-models
    ports:
      - "8002:8002"
    volumes:
      - ./models:/app/models  # æŒä¹…åŒ–æ¨¡å‹å­˜å‚¨
      - ./cache:/root/.cache/huggingface  # ç¼“å­˜ç›®å½•
    environment:
      - TRANSFORMERS_CACHE=/root/.cache/huggingface
      - HF_HOME=/root/.cache/huggingface
```

### æ–¹æ¡ˆäºŒï¼šå¤–éƒ¨å­˜å‚¨æŒ‚è½½ (ç”Ÿäº§æ¨è)

#### 1. åˆ›å»ºæ¨¡å‹å­˜å‚¨ç›®å½•
```bash
# åœ¨å®¿ä¸»æœºåˆ›å»ºæ¨¡å‹ç›®å½•
mkdir -p /Users/szjason72/zervi-basic/basic/models
mkdir -p /Users/szjason72/zervi-basic/basic/cache
```

#### 2. é¢„ä¸‹è½½æ¨¡å‹åˆ°å®¿ä¸»æœº
```bash
# åˆ›å»ºæ¨¡å‹ä¸‹è½½è„šæœ¬
cat > /Users/szjason72/zervi-basic/basic/scripts/download_models.sh << 'EOF'
#!/bin/bash
echo "=== ä¸‹è½½AIæ¨¡å‹åˆ°æœ¬åœ° ==="

# è®¾ç½®ç¯å¢ƒå˜é‡
export HF_HOME="/Users/szjason72/zervi-basic/basic/cache"
export TRANSFORMERS_CACHE="/Users/szjason72/zervi-basic/basic/cache"

# ä¸‹è½½æ ¸å¿ƒæ¨¡å‹
python3 -c "
from sentence_transformers import SentenceTransformer
import os

models = {
    'fast': 'sentence-transformers/all-MiniLM-L6-v2',
    'balanced': 'sentence-transformers/all-MiniLM-L12-v2', 
    'accurate': 'sentence-transformers/all-mpnet-base-v2',
    'multilingual': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
}

for name, model_id in models.items():
    print(f'æ­£åœ¨ä¸‹è½½ {name} æ¨¡å‹: {model_id}')
    try:
        model = SentenceTransformer(model_id)
        print(f'âœ… {name} æ¨¡å‹ä¸‹è½½æˆåŠŸ')
    except Exception as e:
        print(f'âŒ {name} æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}')

print('æ¨¡å‹ä¸‹è½½å®Œæˆï¼')
"

echo "æ¨¡å‹å·²ä¸‹è½½åˆ°: $HF_HOME"
EOF

chmod +x /Users/szjason72/zervi-basic/basic/scripts/download_models.sh
```

#### 3. æ›´æ–°docker-compose.yml
```yaml
services:
  ai-models:
    image: ai-services-ai-models:latest
    ports:
      - "8002:8002"
    volumes:
      - /Users/szjason72/zervi-basic/basic/models:/app/models
      - /Users/szjason72/zervi-basic/basic/cache:/root/.cache/huggingface
    environment:
      - HF_HOME=/root/.cache/huggingface
      - TRANSFORMERS_CACHE=/root/.cache/huggingface
      - MODEL_PATH=/app/models
```

## ğŸ”§ æ¨¡å‹ç®¡ç†æœåŠ¡å®ç°

### 1. åˆ›å»ºæ¨¡å‹ç®¡ç†å™¨
```python
# /app/model_manager.py
import os
import asyncio
from typing import Dict, List, Optional
from sentence_transformers import SentenceTransformer
import logging

logger = logging.getLogger(__name__)

class ModelManager:
    """AIæ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self.models = {}
        self.model_configs = {
            "fast": {
                "name": "sentence-transformers/all-MiniLM-L6-v2",
                "size": "90MB",
                "dimension": 384,
                "use_case": "å®æ—¶åŒ¹é…ï¼Œé«˜å¹¶å‘"
            },
            "balanced": {
                "name": "sentence-transformers/all-MiniLM-L12-v2", 
                "size": "120MB",
                "dimension": 384,
                "use_case": "å¹³è¡¡æ€§èƒ½å’Œç²¾åº¦"
            },
            "accurate": {
                "name": "sentence-transformers/all-mpnet-base-v2",
                "size": "438MB", 
                "dimension": 768,
                "use_case": "é«˜ç²¾åº¦åŒ¹é…"
            },
            "multilingual": {
                "name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                "size": "120MB",
                "dimension": 384,
                "use_case": "å¤šè¯­è¨€æ”¯æŒ"
            }
        }
    
    async def initialize_models(self):
        """åˆå§‹åŒ–å¸¸ç”¨æ¨¡å‹"""
        logger.info("å¼€å§‹åˆå§‹åŒ–AIæ¨¡å‹...")
        
        # é¢„åŠ è½½å¿«é€Ÿæ¨¡å‹
        await self.load_model("fast")
        logger.info("AIæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    async def load_model(self, model_type: str) -> bool:
        """åŠ è½½æŒ‡å®šç±»å‹çš„æ¨¡å‹"""
        if model_type not in self.model_configs:
            logger.error(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_type}")
            return False
        
        if model_type in self.models:
            logger.info(f"æ¨¡å‹ {model_type} å·²åŠ è½½")
            return True
        
        try:
            model_config = self.model_configs[model_type]
            model_name = model_config["name"]
            
            logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
            model = SentenceTransformer(model_name)
            self.models[model_type] = model
            
            logger.info(f"æ¨¡å‹ {model_type} åŠ è½½æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"æ¨¡å‹ {model_type} åŠ è½½å¤±è´¥: {e}")
            return False
    
    async def get_embedding(self, text: str, model_type: str = "fast") -> Optional[List[float]]:
        """è·å–æ–‡æœ¬åµŒå…¥å‘é‡"""
        try:
            # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
            if model_type not in self.models:
                success = await self.load_model(model_type)
                if not success:
                    return None
            
            model = self.models[model_type]
            embedding = model.encode(text)
            
            return embedding.tolist()
            
        except Exception as e:
            logger.error(f"ç”ŸæˆåµŒå…¥å‘é‡å¤±è´¥: {e}")
            return None
    
    async def batch_embedding(self, texts: List[str], model_type: str = "fast") -> List[Optional[List[float]]]:
        """æ‰¹é‡è·å–æ–‡æœ¬åµŒå…¥å‘é‡"""
        try:
            if model_type not in self.models:
                success = await self.load_model(model_type)
                if not success:
                    return [None] * len(texts)
            
            model = self.models[model_type]
            embeddings = model.encode(texts)
            
            return [emb.tolist() for emb in embeddings]
            
        except Exception as e:
            logger.error(f"æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡å¤±è´¥: {e}")
            return [None] * len(texts)
    
    def get_model_info(self, model_type: str) -> Optional[Dict]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return self.model_configs.get(model_type)
    
    def list_available_models(self) -> List[str]:
        """åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹ç±»å‹"""
        return list(self.model_configs.keys())
    
    def get_loaded_models(self) -> List[str]:
        """è·å–å·²åŠ è½½çš„æ¨¡å‹"""
        return list(self.models.keys())

# å…¨å±€æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
model_manager = ModelManager()
```

### 2. æ›´æ–°AI ModelsæœåŠ¡
```python
# åœ¨ai_models_service.pyä¸­é›†æˆæ¨¡å‹ç®¡ç†å™¨
from model_manager import model_manager

class AIModelsService:
    def __init__(self):
        self.model_path = os.getenv("MODEL_PATH", "/app/models")
        self.max_memory = os.getenv("MAX_MEMORY", "3GB")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        asyncio.create_task(model_manager.initialize_models())
        
        logger.info("AIæ¨¡å‹æœåŠ¡åˆå§‹åŒ–", device=self.device, model_path=self.model_path)
    
    async def get_embedding(self, text: str, model_type: str = "fast") -> Optional[List[float]]:
        """è·å–æ–‡æœ¬åµŒå…¥å‘é‡"""
        return await model_manager.get_embedding(text, model_type)
    
    async def batch_embedding(self, texts: List[str], model_type: str = "fast") -> List[Optional[List[float]]]:
        """æ‰¹é‡è·å–æ–‡æœ¬åµŒå…¥å‘é‡"""
        return await model_manager.batch_embedding(texts, model_type)
```

### 3. æ·»åŠ æ¨¡å‹ç®¡ç†API
```python
# åœ¨ai_models_service.pyä¸­æ·»åŠ æ–°çš„APIç«¯ç‚¹

@app.route("/api/v1/models/info", methods=["GET"])
async def get_model_info(request: Request):
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    model_type = request.args.get("type", "fast")
    info = model_manager.get_model_info(model_type)
    
    if info:
        return json({
            "status": "success",
            "model_type": model_type,
            "info": info
        })
    else:
        return json({
            "status": "error",
            "message": f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_type}"
        }, status=400)

@app.route("/api/v1/models/list", methods=["GET"])
async def list_models(request: Request):
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
    available = model_manager.list_available_models()
    loaded = model_manager.get_loaded_models()
    
    return json({
        "status": "success",
        "available_models": available,
        "loaded_models": loaded,
        "count": len(available)
    })

@app.route("/api/v1/models/load", methods=["POST"])
async def load_model(request: Request):
    """åŠ è½½æŒ‡å®šæ¨¡å‹"""
    try:
        data = request.json
        model_type = data.get("model_type", "fast")
        
        success = await model_manager.load_model(model_type)
        
        if success:
            return json({
                "status": "success",
                "message": f"æ¨¡å‹ {model_type} åŠ è½½æˆåŠŸ",
                "model_type": model_type
            })
        else:
            return json({
                "status": "error",
                "message": f"æ¨¡å‹ {model_type} åŠ è½½å¤±è´¥"
            }, status=500)
            
    except Exception as e:
        logger.error("åŠ è½½æ¨¡å‹å¤±è´¥", error=str(e))
        return json({"error": str(e)}, status=500)
```

## ğŸ“‹ å®æ–½æ­¥éª¤

### ç¬¬ä¸€æ­¥ï¼šä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ° (30åˆ†é’Ÿ)
```bash
# 1. åˆ›å»ºæ¨¡å‹ç›®å½•
mkdir -p /Users/szjason72/zervi-basic/basic/models
mkdir -p /Users/szjason72/zervi-basic/basic/cache

# 2. æ‰§è¡Œæ¨¡å‹ä¸‹è½½è„šæœ¬
cd /Users/szjason72/zervi-basic/basic
./scripts/download_models.sh

# 3. éªŒè¯ä¸‹è½½ç»“æœ
ls -la /Users/szjason72/zervi-basic/basic/cache/
```

### ç¬¬äºŒæ­¥ï¼šæ›´æ–°Dockeré…ç½® (15åˆ†é’Ÿ)
```bash
# 1. æ›´æ–°docker-compose.yml
# 2. é‡æ–°æ„å»ºAI ModelsæœåŠ¡
docker-compose build ai-models

# 3. é‡å¯AIæœåŠ¡
docker-compose restart ai-models
```

### ç¬¬ä¸‰æ­¥ï¼šæµ‹è¯•æ¨¡å‹æœåŠ¡ (15åˆ†é’Ÿ)
```bash
# 1. æµ‹è¯•æ¨¡å‹åˆ—è¡¨API
curl http://localhost:8002/api/v1/models/list

# 2. æµ‹è¯•åµŒå…¥ç”ŸæˆAPI
curl -X POST http://localhost:8002/api/v1/models/embedding \
  -H "Content-Type: application/json" \
  -d '{"text": "Pythonå¼€å‘å·¥ç¨‹å¸ˆ", "model_type": "fast"}'

# 3. æµ‹è¯•æ¨¡å‹ä¿¡æ¯API
curl http://localhost:8002/api/v1/models/info?type=fast
```

## ğŸ’° æˆæœ¬ä¼°ç®—

### å­˜å‚¨æˆæœ¬
- **æ¨¡å‹æ–‡ä»¶**: çº¦1GB (4ä¸ªæ ¸å¿ƒæ¨¡å‹)
- **ç¼“å­˜æ–‡ä»¶**: çº¦500MB
- **æ€»è®¡**: çº¦1.5GBå­˜å‚¨ç©ºé—´

### å†…å­˜æˆæœ¬
- **å•ä¸ªæ¨¡å‹**: 200-800MBå†…å­˜
- **å¹¶å‘åŠ è½½**: æœ€å¤š3ä¸ªæ¨¡å‹ = 1-2GBå†…å­˜
- **æ¨èé…ç½®**: 4GBå†…å­˜è¶³å¤Ÿ

### ç½‘ç»œæˆæœ¬
- **é¦–æ¬¡ä¸‹è½½**: 1.5GBæ•°æ®
- **åç»­æ›´æ–°**: æŒ‰éœ€ä¸‹è½½
- **å»ºè®®**: ä½¿ç”¨æœ¬åœ°ç½‘ç»œæˆ–CDN

## ğŸ¯ æ€»ç»“

**å›ç­”æ‚¨çš„é—®é¢˜**: æ˜¯çš„ï¼Œæ¨¡å‹éœ€è¦ä¸‹è½½åˆ°æœ¬åœ°ï¼Œä½†è¿™æ˜¯å¿…è¦çš„æŠ•èµ„ï¼š

1. **æ€§èƒ½æå‡**: æœ¬åœ°æ¨¡å‹å“åº”æ—¶é—´<100ms vs åœ¨çº¿API>1s
2. **ç¨³å®šæ€§**: é¿å…ç½‘ç»œæ³¢åŠ¨å½±å“æœåŠ¡
3. **æˆæœ¬æ§åˆ¶**: é¿å…é¢‘ç¹APIè°ƒç”¨è´¹ç”¨
4. **æ•°æ®å®‰å…¨**: æ•æ„Ÿæ•°æ®ä¸ç¦»å¼€æœ¬åœ°

**æ¨èæ–¹æ¡ˆ**: ä½¿ç”¨å¤–éƒ¨å­˜å‚¨æŒ‚è½½ + é¢„ä¸‹è½½æ ¸å¿ƒæ¨¡å‹
**é¢„æœŸæ•ˆæœ**: æ”¯æŒ4ä¸ªæ ¸å¿ƒæ¨¡å‹ï¼Œå“åº”æ—¶é—´<100msï¼Œå‡†ç¡®ç‡>85%

é€šè¿‡è¿™ä¸ªæ–¹æ¡ˆï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºä¸€ä¸ªå¼ºå¤§ã€ç¨³å®šã€é«˜æ•ˆçš„AIæ¨¡å‹æœåŠ¡ï¼
