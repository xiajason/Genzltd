# AIå¤§æ¨¡å‹æ”¯æŒç­–ç•¥

## ğŸ“‹ å½“å‰çŠ¶æ€åˆ†æ

### å·²å®‰è£…çš„æ¨¡å‹
- **å½“å‰æ¨¡å‹**: `sentence-transformers/all-MiniLM-L6-v2`
- **æ¨¡å‹å¤§å°**: 909MB (å·²ä¸‹è½½åˆ°å®¹å™¨)
- **æ¨¡å‹ç»´åº¦**: 384ç»´
- **æ¨¡å‹ç±»å‹**: å¥å­åµŒå…¥æ¨¡å‹ (Sentence Embedding)
- **ç”¨é€”**: æ–‡æœ¬è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—

### æŠ€æœ¯æ ˆ
- **Transformers**: 4.30.2 (AIæœåŠ¡) / 4.36.2 (AI ModelsæœåŠ¡)
- **PyTorch**: 2.0.1 (AIæœåŠ¡) / 2.1.2 (AI ModelsæœåŠ¡)
- **Sentence-Transformers**: 2.2.2
- **CUDAæ”¯æŒ**: å¦ (CPUæ¨¡å¼)

## ğŸ¯ å¤§æ¨¡å‹éƒ¨ç½²ç­–ç•¥

### 1. æ¨¡å‹ä¸‹è½½æ–¹å¼

#### æ–¹å¼ä¸€ï¼šå®¹å™¨å†…åŠ¨æ€ä¸‹è½½ (å½“å‰æ–¹å¼)
```python
# ä¼˜ç‚¹ï¼šæŒ‰éœ€ä¸‹è½½ï¼ŒèŠ‚çœå­˜å‚¨ç©ºé—´
# ç¼ºç‚¹ï¼šé¦–æ¬¡ä½¿ç”¨éœ€è¦ç½‘ç»œä¸‹è½½ï¼Œå¯èƒ½å¤±è´¥
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
```

#### æ–¹å¼äºŒï¼šé¢„ä¸‹è½½åˆ°å®¹å™¨é•œåƒ
```dockerfile
# åœ¨Dockerfileä¸­é¢„ä¸‹è½½æ¨¡å‹
RUN python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
```

#### æ–¹å¼ä¸‰ï¼šæ¨¡å‹æ–‡ä»¶æŒ‚è½½
```yaml
# docker-compose.yml
volumes:
  - ./models:/app/models
  - ~/.cache/huggingface:/root/.cache/huggingface
```

### 2. æ¨èçš„å¤§æ¨¡å‹é…ç½®

#### è½»é‡çº§æ¨¡å‹ (æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒ)
```python
LIGHTWEIGHT_MODELS = {
    "all-MiniLM-L6-v2": {
        "size": "90MB",
        "dimension": 384,
        "speed": "fast",
        "accuracy": "good",
        "use_case": "å®æ—¶åŒ¹é…ï¼Œé«˜å¹¶å‘"
    },
    "all-MiniLM-L12-v2": {
        "size": "120MB", 
        "dimension": 384,
        "speed": "fast",
        "accuracy": "better",
        "use_case": "å¹³è¡¡æ€§èƒ½å’Œç²¾åº¦"
    }
}
```

#### é«˜æ€§èƒ½æ¨¡å‹ (æ¨èç”¨äºé«˜ç²¾åº¦åœºæ™¯)
```python
HIGH_PERFORMANCE_MODELS = {
    "all-mpnet-base-v2": {
        "size": "438MB",
        "dimension": 768,
        "speed": "medium",
        "accuracy": "excellent",
        "use_case": "é«˜ç²¾åº¦åŒ¹é…ï¼Œç¦»çº¿åˆ†æ"
    },
    "paraphrase-multilingual-MiniLM-L12-v2": {
        "size": "120MB",
        "dimension": 384,
        "speed": "fast", 
        "accuracy": "good",
        "use_case": "å¤šè¯­è¨€æ”¯æŒ"
    }
}
```

#### ä¸“ä¸šé¢†åŸŸæ¨¡å‹ (æ¨èç”¨äºç‰¹å®šåœºæ™¯)
```python
DOMAIN_SPECIFIC_MODELS = {
    "all-distilroberta-v1": {
        "size": "290MB",
        "dimension": 768,
        "speed": "medium",
        "accuracy": "excellent",
        "use_case": "æŠ€æœ¯æ–‡æ¡£åŒ¹é…"
    },
    "msmarco-distilbert-base-v4": {
        "size": "290MB",
        "dimension": 768,
        "speed": "medium", 
        "accuracy": "excellent",
        "use_case": "æœç´¢å’Œæ£€ç´¢"
    }
}
```

## ğŸš€ æœªæ¥åŒ¹é…æœåŠ¡å¤§æ¨¡å‹æ”¯æŒæ–¹æ¡ˆ

### Phase 1: åŸºç¡€æ¨¡å‹æ”¯æŒ (1-2å‘¨)

#### ç›®æ ‡
- æ”¯æŒ3-5ä¸ªæ ¸å¿ƒæ¨¡å‹
- å®ç°æ¨¡å‹åŠ¨æ€åˆ‡æ¢
- å»ºç«‹æ¨¡å‹æ€§èƒ½ç›‘æ§

#### å®ç°æ–¹æ¡ˆ
```python
class ModelManager:
    def __init__(self):
        self.available_models = {
            "fast": "sentence-transformers/all-MiniLM-L6-v2",
            "balanced": "sentence-transformers/all-MiniLM-L12-v2", 
            "accurate": "sentence-transformers/all-mpnet-base-v2"
        }
        self.loaded_models = {}
    
    async def get_embedding(self, text: str, model_type: str = "fast"):
        """æ ¹æ®éœ€æ±‚é€‰æ‹©æ¨¡å‹"""
        model_name = self.available_models.get(model_type)
        if not model_name:
            raise ValueError(f"Unknown model type: {model_type}")
        
        if model_name not in self.loaded_models:
            self.loaded_models[model_name] = SentenceTransformer(model_name)
        
        return self.loaded_models[model_name].encode(text)
```

### Phase 2: å¤šæ¨¡å‹é›†æˆ (2-4å‘¨)

#### ç›®æ ‡
- æ”¯æŒ10+ä¸ªä¸“ä¸šæ¨¡å‹
- å®ç°æ¨¡å‹ç»„åˆä½¿ç”¨
- æ·»åŠ æ¨¡å‹æ€§èƒ½è¯„ä¼°

#### æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
```python
SUPPORTED_MODELS = {
    # é€šç”¨åµŒå…¥æ¨¡å‹
    "general": [
        "sentence-transformers/all-MiniLM-L6-v2",
        "sentence-transformers/all-MiniLM-L12-v2",
        "sentence-transformers/all-mpnet-base-v2",
        "sentence-transformers/all-distilroberta-v1"
    ],
    
    # å¤šè¯­è¨€æ¨¡å‹
    "multilingual": [
        "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    ],
    
    # ä¸“ä¸šé¢†åŸŸæ¨¡å‹
    "domain_specific": [
        "sentence-transformers/msmarco-distilbert-base-v4",
        "sentence-transformers/all-nli-roberta-large-v1",
        "sentence-transformers/paraphrase-MiniLM-L6-v2"
    ],
    
    # ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹
    "chinese": [
        "shibing624/text2vec-base-chinese",
        "shibing624/text2vec-base-chinese-sentence",
        "uer/sbert-base-chinese-nli"
    ]
}
```

### Phase 3: é«˜çº§åŠŸèƒ½ (1-2ä¸ªæœˆ)

#### ç›®æ ‡
- æ”¯æŒå¤§è¯­è¨€æ¨¡å‹ (LLM)
- å®ç°æ··åˆæ¨¡å‹æ¶æ„
- æ·»åŠ æ¨¡å‹å¾®è°ƒèƒ½åŠ›

#### å¤§è¯­è¨€æ¨¡å‹æ”¯æŒ
```python
LLM_MODELS = {
    # å¼€æºæ¨¡å‹
    "open_source": [
        "microsoft/DialoGPT-medium",
        "microsoft/DialoGPT-large", 
        "facebook/blenderbot-400M-distill",
        "facebook/blenderbot-1B-distill"
    ],
    
    # ä¸­æ–‡æ¨¡å‹
    "chinese_llm": [
        "THUDM/chatglm-6b",
        "THUDM/chatglm2-6b",
        "baichuan-inc/Baichuan-7B-Chat",
        "Qwen/Qwen-7B-Chat"
    ],
    
    # ä¸“ä¸šæ¨¡å‹
    "professional": [
        "microsoft/DialoGPT-medium",
        "facebook/blenderbot-400M-distill"
    ]
}
```

## ğŸ’¾ å­˜å‚¨å’Œéƒ¨ç½²ç­–ç•¥

### 1. æ¨¡å‹å­˜å‚¨æ–¹æ¡ˆ

#### æ–¹æ¡ˆAï¼šå®¹å™¨å†…å­˜å‚¨ (å½“å‰)
```dockerfile
# ä¼˜ç‚¹ï¼šç®€å•ï¼Œè‡ªåŒ…å«
# ç¼ºç‚¹ï¼šé•œåƒå¤§ï¼Œæ›´æ–°å›°éš¾
FROM python:3.11
COPY models/ /app/models/
```

#### æ–¹æ¡ˆBï¼šå¤–éƒ¨å­˜å‚¨æŒ‚è½½ (æ¨è)
```yaml
# docker-compose.yml
services:
  ai-models:
    volumes:
      - ./models:/app/models
      - ./cache:/root/.cache/huggingface
    environment:
      - TRANSFORMERS_CACHE=/app/cache
```

#### æ–¹æ¡ˆCï¼šäº‘å­˜å‚¨é›†æˆ
```python
# æ”¯æŒä»äº‘å­˜å‚¨ä¸‹è½½æ¨¡å‹
import boto3
from huggingface_hub import hf_hub_download

class CloudModelManager:
    def __init__(self):
        self.s3_client = boto3.client('s3')
    
    async def download_model(self, model_name: str):
        # ä»S3ä¸‹è½½æ¨¡å‹æ–‡ä»¶
        pass
```

### 2. æ¨¡å‹ç¼“å­˜ç­–ç•¥

#### å†…å­˜ç¼“å­˜
```python
from functools import lru_cache

@lru_cache(maxsize=5)
def load_model(model_name: str):
    return SentenceTransformer(model_name)
```

#### ç£ç›˜ç¼“å­˜
```python
import os
from pathlib import Path

CACHE_DIR = Path("/app/cache/models")

def get_cached_model(model_name: str):
    cache_path = CACHE_DIR / model_name
    if cache_path.exists():
        return SentenceTransformer(str(cache_path))
    return None
```

## ğŸ”§ æŠ€æœ¯å®ç°å»ºè®®

### 1. æ¨¡å‹æœåŠ¡æ¶æ„

```python
class AIModelService:
    def __init__(self):
        self.model_pool = {}
        self.model_configs = self.load_model_configs()
    
    async def initialize_models(self):
        """é¢„åŠ è½½å¸¸ç”¨æ¨¡å‹"""
        for model_type, model_name in self.model_configs.items():
            if model_type in ["fast", "balanced"]:
                await self.load_model(model_name)
    
    async def get_embedding(self, text: str, model_type: str = "fast"):
        """è·å–æ–‡æœ¬åµŒå…¥"""
        model = await self.get_model(model_type)
        return model.encode(text)
    
    async def batch_embedding(self, texts: List[str], model_type: str = "fast"):
        """æ‰¹é‡è·å–åµŒå…¥"""
        model = await self.get_model(model_type)
        return model.encode(texts)
```

### 2. æ€§èƒ½ä¼˜åŒ–

#### æ¨¡å‹é‡åŒ–
```python
# ä½¿ç”¨é‡åŒ–æ¨¡å‹å‡å°‘å†…å­˜ä½¿ç”¨
from transformers import AutoModel, AutoTokenizer
import torch

def load_quantized_model(model_name: str):
    model = AutoModel.from_pretrained(
        model_name,
        torch_dtype=torch.float16,  # åŠç²¾åº¦
        device_map="auto"
    )
    return model
```

#### æ‰¹å¤„ç†ä¼˜åŒ–
```python
async def batch_process(texts: List[str], batch_size: int = 32):
    """æ‰¹é‡å¤„ç†æ–‡æœ¬"""
    results = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        embeddings = await get_embeddings(batch)
        results.extend(embeddings)
    return results
```

### 3. ç›‘æ§å’Œæ—¥å¿—

```python
import time
import logging

class ModelMonitor:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.metrics = {}
    
    def log_model_usage(self, model_name: str, duration: float, text_length: int):
        """è®°å½•æ¨¡å‹ä½¿ç”¨æƒ…å†µ"""
        self.metrics[model_name] = {
            "usage_count": self.metrics.get(model_name, {}).get("usage_count", 0) + 1,
            "total_duration": self.metrics.get(model_name, {}).get("total_duration", 0) + duration,
            "total_text_length": self.metrics.get(model_name, {}).get("total_text_length", 0) + text_length
        }
```

## ğŸ“Š æˆæœ¬åˆ†æ

### å­˜å‚¨æˆæœ¬
- **è½»é‡çº§æ¨¡å‹**: 100-200MB Ã— 5ä¸ª = 500MB-1GB
- **é«˜æ€§èƒ½æ¨¡å‹**: 400-500MB Ã— 3ä¸ª = 1.2-1.5GB  
- **æ€»è®¡**: çº¦2-3GBå­˜å‚¨ç©ºé—´

### å†…å­˜æˆæœ¬
- **å•ä¸ªæ¨¡å‹**: 200-800MBå†…å­˜
- **å¹¶å‘åŠ è½½**: æœ€å¤š3-5ä¸ªæ¨¡å‹ = 1-4GBå†…å­˜
- **æ¨èé…ç½®**: 8GBå†…å­˜æœåŠ¡å™¨

### ç½‘ç»œæˆæœ¬
- **é¦–æ¬¡ä¸‹è½½**: 2-3GBæ•°æ®
- **æ¨¡å‹æ›´æ–°**: æŒ‰éœ€ä¸‹è½½
- **å»ºè®®**: ä½¿ç”¨CDNåŠ é€Ÿä¸‹è½½

## ğŸ¯ å®æ–½å»ºè®®

### çŸ­æœŸ (1-2å‘¨)
1. **ä¼˜åŒ–å½“å‰æ¨¡å‹**: ç¡®ä¿`all-MiniLM-L6-v2`ç¨³å®šè¿è¡Œ
2. **æ·»åŠ æ¨¡å‹åˆ‡æ¢**: æ”¯æŒfast/balanced/accurateä¸‰ç§æ¨¡å¼
3. **æ€§èƒ½ç›‘æ§**: æ·»åŠ æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡å’Œæ€§èƒ½æŒ‡æ ‡

### ä¸­æœŸ (1-2ä¸ªæœˆ)  
1. **å¤šæ¨¡å‹æ”¯æŒ**: é›†æˆ5-10ä¸ªä¸“ä¸šæ¨¡å‹
2. **ç¼“å­˜ä¼˜åŒ–**: å®ç°æ™ºèƒ½æ¨¡å‹ç¼“å­˜å’Œé¢„åŠ è½½
3. **APIä¼˜åŒ–**: æ”¯æŒæ‰¹é‡å¤„ç†å’Œå¼‚æ­¥è°ƒç”¨

### é•¿æœŸ (3-6ä¸ªæœˆ)
1. **LLMé›†æˆ**: æ”¯æŒå¤§è¯­è¨€æ¨¡å‹ç”¨äºå¤æ‚åŒ¹é…
2. **æ¨¡å‹å¾®è°ƒ**: åŸºäºä¸šåŠ¡æ•°æ®å¾®è°ƒæ¨¡å‹
3. **äº‘åŸç”Ÿ**: æ”¯æŒKuberneteséƒ¨ç½²å’Œè‡ªåŠ¨æ‰©ç¼©å®¹

## ğŸ” æ€»ç»“

**å½“å‰çŠ¶æ€**: å·²å®‰è£…åŸºç¡€æ¨¡å‹ï¼Œéœ€è¦ä¼˜åŒ–éƒ¨ç½²ç­–ç•¥
**æ¨èæ–¹æ¡ˆ**: ä½¿ç”¨å¤–éƒ¨å­˜å‚¨æŒ‚è½½ + æ¨¡å‹ç¼“å­˜ + å¤šæ¨¡å‹æ”¯æŒ
**é¢„æœŸæ•ˆæœ**: æ”¯æŒ10+æ¨¡å‹ï¼Œå“åº”æ—¶é—´<100msï¼Œå‡†ç¡®ç‡>85%

é€šè¿‡åˆ†é˜¶æ®µå®æ–½ï¼Œå¯ä»¥é€æ­¥æ„å»ºå¼ºå¤§çš„AIæ¨¡å‹æœåŠ¡ï¼Œä¸ºJobFirstå¹³å°æä¾›ç²¾å‡†çš„èŒä½åŒ¹é…èƒ½åŠ›ã€‚
