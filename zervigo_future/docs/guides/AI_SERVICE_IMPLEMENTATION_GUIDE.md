# JobFirst AIæœåŠ¡å…·ä½“æ¥å…¥å®ç°æŒ‡å—

**è®¾è®¡æ—¥æœŸ**: 2025å¹´1æœˆ6æ—¥  
**ç›®æ ‡**: åŸºäºç°æœ‰AIæœåŠ¡æ¶æ„ï¼Œå®Œå–„ä¼ä¸šå’ŒèŒä½ç›¸å…³çš„å¤§æ¨¡å‹æœåŠ¡å…·ä½“æ¥å…¥æ–¹æ¡ˆ  
**åŸºç¡€**: åŸºäºAI_SERVICE_DATABASE_UPGRADE.mdå’Œç°æœ‰ai_service.pyæ¶æ„  

## ğŸ“‹ å®ç°æ¦‚è¿°

æœ¬æŒ‡å—åŸºäºç°æœ‰çš„Python Sanic AIæœåŠ¡æ¶æ„ï¼Œæä¾›å…·ä½“çš„ä¼ä¸šå’ŒèŒä½AIæœåŠ¡æ¥å…¥å®ç°æ–¹æ¡ˆï¼ŒåŒ…æ‹¬APIæ¥å£è®¾è®¡ã€æ•°æ®åº“é›†æˆã€æ¨¡å‹è°ƒç”¨ç­‰å…·ä½“å®ç°ç»†èŠ‚ã€‚

## ğŸ—ï¸ ç°æœ‰æ¶æ„åˆ†æ

### å½“å‰AIæœåŠ¡æ¶æ„
- **æ¡†æ¶**: Python Sanic
- **ç«¯å£**: 8206
- **æ•°æ®åº“**: PostgreSQL (jobfirst_vector)
- **AIæ¨¡å‹**: Ollama (gemma3:4b)
- **æ ¸å¿ƒåŠŸèƒ½**: ç®€å†åˆ†æã€å‘é‡ç”Ÿæˆã€ç›¸ä¼¼åº¦æœç´¢

### ç°æœ‰APIç«¯ç‚¹
- `GET /health` - å¥åº·æ£€æŸ¥
- `POST /api/v1/analyze/resume` - ç®€å†åˆ†æ
- `GET /api/v1/vectors/<resume_id>` - è·å–ç®€å†å‘é‡
- `POST /api/v1/vectors/search` - æœç´¢ç›¸ä¼¼ç®€å†

## ğŸš€ ä¼ä¸šAIæœåŠ¡æ¥å…¥å®ç°

### 1. ä¼ä¸šåˆ†æAPIå®ç°

#### 1.1 ä¼ä¸šç”»åƒç”ŸæˆAPI
```python
@app.route("/api/v1/analyze/company", methods=["POST"])
async def analyze_company(request: Request):
    """åˆ†æä¼ä¸šä¿¡æ¯å¹¶ç”Ÿæˆä¼ä¸šç”»åƒ"""
    try:
        data = request.json
        company_id = data.get("company_id")
        company_data = data.get("company_data", {})
        
        logger.info(f"å¼€å§‹åˆ†æä¼ä¸š: {company_id}")
        
        # æ‰§è¡Œä¼ä¸šAIåˆ†æ
        analysis = await perform_company_analysis(company_data)
        
        # ç”Ÿæˆä¼ä¸šåµŒå…¥å‘é‡
        embeddings = await generate_company_embeddings(company_data, analysis)
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        await save_company_analysis_to_db(company_id, analysis, embeddings)
        
        response = {
            "company_id": company_id,
            "status": "completed",
            "analysis": {
                "company_profile": analysis.company_profile,
                "culture_analysis": analysis.culture_analysis,
                "benefits_analysis": analysis.benefits_analysis,
                "growth_potential": analysis.growth_potential,
                "industry_position": analysis.industry_position,
                "confidence_score": analysis.confidence_score
            },
            "embeddings": {
                "description_vector": embeddings.description_vector,
                "culture_vector": embeddings.culture_vector,
                "benefits_vector": embeddings.benefits_vector,
                "overall_vector": embeddings.overall_vector
            },
            "created_at": datetime.now().isoformat()
        }
        
        logger.info(f"ä¼ä¸šåˆ†æå®Œæˆ: {company_id}")
        return sanic_response(response)
        
    except Exception as e:
        logger.error(f"ä¼ä¸šåˆ†æå¤±è´¥: {e}")
        return sanic_response({"error": str(e)}, status=500)
```

#### 1.2 ä¼ä¸šåˆ†ææ•°æ®æ¨¡å‹
```python
class CompanyAnalysisRequest:
    def __init__(self, company_id: int, company_data: Dict[str, Any]):
        self.company_id = company_id
        self.company_data = company_data

class CompanyAnalysis:
    def __init__(self, company_profile: str, culture_analysis: str, 
                 benefits_analysis: str, growth_potential: str, 
                 industry_position: str, confidence_score: float):
        self.company_profile = company_profile
        self.culture_analysis = culture_analysis
        self.benefits_analysis = benefits_analysis
        self.growth_potential = growth_potential
        self.industry_position = industry_position
        self.confidence_score = confidence_score

class CompanyEmbeddings:
    def __init__(self, description_vector: List[float], culture_vector: List[float], 
                 benefits_vector: List[float], overall_vector: List[float]):
        self.description_vector = description_vector
        self.culture_vector = culture_vector
        self.benefits_vector = benefits_vector
        self.overall_vector = overall_vector
```

#### 1.3 ä¼ä¸šAIåˆ†æå‡½æ•°
```python
async def perform_company_analysis(company_data: Dict[str, Any]) -> CompanyAnalysis:
    """æ‰§è¡Œä¼ä¸šAIåˆ†æ"""
    try:
        # æ„å»ºä¼ä¸šåˆ†ææç¤ºè¯
        prompt = f"""è¯·åˆ†æä»¥ä¸‹ä¼ä¸šä¿¡æ¯ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š

ä¼ä¸šä¿¡æ¯ï¼š
- åç§°: {company_data.get('name', '')}
- è¡Œä¸š: {company_data.get('industry', '')}
- è§„æ¨¡: {company_data.get('size', '')}
- ä½ç½®: {company_data.get('location', '')}
- æè¿°: {company_data.get('description', '')}
- ç½‘ç«™: {company_data.get('website', '')}

è¯·åˆ†æå¹¶è¿”å›ä»¥ä¸‹ä¿¡æ¯ï¼ˆJSONæ ¼å¼ï¼‰ï¼š
{{
    "company_profile": "ä¼ä¸šæ•´ä½“ç”»åƒæè¿°",
    "culture_analysis": "ä¼ä¸šæ–‡åŒ–åˆ†æ",
    "benefits_analysis": "ä¼ä¸šç¦åˆ©å¾…é‡åˆ†æ",
    "growth_potential": "ä¼ä¸šå‘å±•æ½œåŠ›è¯„ä¼°",
    "industry_position": "è¡Œä¸šåœ°ä½åˆ†æ",
    "confidence_score": 0.85
}}

è¯·ç¡®ä¿è¿”å›çš„æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚"""

        # è°ƒç”¨Ollama API
        response = requests.post(f"{Config.OLLAMA_HOST}/api/generate", json={
            "model": Config.OLLAMA_MODEL,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.3,
                "top_p": 0.9,
                "max_tokens": 1500
            }
        })
        
        if response.status_code == 200:
            ai_response = response.json()["response"]
            logger.info(f"ä¼ä¸šåˆ†æOllamaå“åº”: {ai_response}")
            
            # è§£æJSONå“åº”
            try:
                json_start = ai_response.find('{')
                json_end = ai_response.rfind('}') + 1
                if json_start != -1 and json_end != 0:
                    json_str = ai_response[json_start:json_end]
                    parsed_data = json.loads(json_str)
                    
                    return CompanyAnalysis(
                        company_profile=parsed_data.get("company_profile", ""),
                        culture_analysis=parsed_data.get("culture_analysis", ""),
                        benefits_analysis=parsed_data.get("benefits_analysis", ""),
                        growth_potential=parsed_data.get("growth_potential", ""),
                        industry_position=parsed_data.get("industry_position", ""),
                        confidence_score=parsed_data.get("confidence_score", 0.7)
                    )
                else:
                    raise ValueError("æœªæ‰¾åˆ°JSONæ ¼å¼")
                    
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"ä¼ä¸šåˆ†æJSONè§£æå¤±è´¥: {e}, ä½¿ç”¨é™çº§åˆ†æ")
                return get_fallback_company_analysis(company_data)
        else:
            logger.error(f"ä¼ä¸šåˆ†æOllama APIè°ƒç”¨å¤±è´¥: {response.status_code}")
            return get_fallback_company_analysis(company_data)
            
    except Exception as e:
        logger.error(f"ä¼ä¸šAIåˆ†æå¤±è´¥: {e}, ä½¿ç”¨é™çº§åˆ†æ")
        return get_fallback_company_analysis(company_data)

def get_fallback_company_analysis(company_data: Dict[str, Any]) -> CompanyAnalysis:
    """é™çº§ä¼ä¸šåˆ†æ"""
    name = company_data.get('name', 'æœªçŸ¥ä¼ä¸š')
    industry = company_data.get('industry', 'æœªçŸ¥è¡Œä¸š')
    size = company_data.get('size', 'æœªçŸ¥è§„æ¨¡')
    
    return CompanyAnalysis(
        company_profile=f"{name}æ˜¯ä¸€å®¶ä¸“æ³¨äº{industry}çš„{size}ä¼ä¸š",
        culture_analysis="ä¼ä¸šæ–‡åŒ–æ³¨é‡åˆ›æ–°å’Œå›¢é˜Ÿåˆä½œ",
        benefits_analysis="æä¾›å…·æœ‰ç«äº‰åŠ›çš„è–ªé…¬ç¦åˆ©",
        growth_potential="å…·æœ‰è‰¯å¥½çš„å‘å±•å‰æ™¯",
        industry_position=f"åœ¨{industry}é¢†åŸŸå…·æœ‰ä¸€å®šå½±å“åŠ›",
        confidence_score=0.6
    )
```

### 2. èŒä½AIæœåŠ¡æ¥å…¥å®ç°

#### 2.1 èŒä½åˆ†æAPIå®ç°
```python
@app.route("/api/v1/analyze/job", methods=["POST"])
async def analyze_job(request: Request):
    """åˆ†æèŒä½ä¿¡æ¯å¹¶ç”ŸæˆèŒä½ç”»åƒ"""
    try:
        data = request.json
        job_id = data.get("job_id")
        job_data = data.get("job_data", {})
        
        logger.info(f"å¼€å§‹åˆ†æèŒä½: {job_id}")
        
        # æ‰§è¡ŒèŒä½AIåˆ†æ
        analysis = await perform_job_analysis(job_data)
        
        # ç”ŸæˆèŒä½åµŒå…¥å‘é‡
        embeddings = await generate_job_embeddings(job_data, analysis)
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        await save_job_analysis_to_db(job_id, analysis, embeddings)
        
        response = {
            "job_id": job_id,
            "status": "completed",
            "analysis": {
                "enhanced_description": analysis.enhanced_description,
                "extracted_skills": analysis.extracted_skills,
                "salary_prediction": analysis.salary_prediction,
                "experience_requirements": analysis.experience_requirements,
                "company_culture_fit": analysis.company_culture_fit,
                "confidence_score": analysis.confidence_score
            },
            "embeddings": {
                "title_vector": embeddings.title_vector,
                "description_vector": embeddings.description_vector,
                "requirements_vector": embeddings.requirements_vector,
                "overall_vector": embeddings.overall_vector
            },
            "created_at": datetime.now().isoformat()
        }
        
        logger.info(f"èŒä½åˆ†æå®Œæˆ: {job_id}")
        return sanic_response(response)
        
    except Exception as e:
        logger.error(f"èŒä½åˆ†æå¤±è´¥: {e}")
        return sanic_response({"error": str(e)}, status=500)
```

#### 2.2 èŒä½åˆ†ææ•°æ®æ¨¡å‹
```python
class JobAnalysisRequest:
    def __init__(self, job_id: int, job_data: Dict[str, Any]):
        self.job_id = job_id
        self.job_data = job_data

class JobAnalysis:
    def __init__(self, enhanced_description: str, extracted_skills: List[str], 
                 salary_prediction: Dict[str, Any], experience_requirements: str,
                 company_culture_fit: str, confidence_score: float):
        self.enhanced_description = enhanced_description
        self.extracted_skills = extracted_skills
        self.salary_prediction = salary_prediction
        self.experience_requirements = experience_requirements
        self.company_culture_fit = company_culture_fit
        self.confidence_score = confidence_score

class JobEmbeddings:
    def __init__(self, title_vector: List[float], description_vector: List[float], 
                 requirements_vector: List[float], overall_vector: List[float]):
        self.title_vector = title_vector
        self.description_vector = description_vector
        self.requirements_vector = requirements_vector
        self.overall_vector = overall_vector
```

### 3. æ™ºèƒ½æ¨èæœåŠ¡æ¥å…¥å®ç°

#### 3.1 èŒä½æ¨èAPI
```python
@app.route("/api/v1/recommend/jobs", methods=["POST"])
async def recommend_jobs(request: Request):
    """ä¸ºç”¨æˆ·æ¨èèŒä½"""
    try:
        data = request.json
        user_id = data.get("user_id")
        limit = data.get("limit", 10)
        filters = data.get("filters", {})
        
        logger.info(f"å¼€å§‹ä¸ºç”¨æˆ·æ¨èèŒä½: {user_id}")
        
        # è·å–ç”¨æˆ·ç”»åƒ
        user_profile = await get_user_profile(user_id)
        
        # æ‰§è¡Œæ™ºèƒ½æ¨è
        recommendations = await perform_job_recommendation(user_id, user_profile, limit, filters)
        
        # ä¿å­˜æ¨èç»“æœ
        await save_job_recommendations_to_db(user_id, recommendations)
        
        response = {
            "user_id": user_id,
            "recommendations": [
                {
                    "job_id": rec.job_id,
                    "recommendation_score": rec.score,
                    "match_reasons": rec.match_reasons,
                    "match_factors": rec.match_factors
                } for rec in recommendations
            ],
            "total": len(recommendations),
            "generated_at": datetime.now().isoformat()
        }
        
        logger.info(f"èŒä½æ¨èå®Œæˆ: {user_id}, æ¨èæ•°é‡: {len(recommendations)}")
        return sanic_response(response)
        
    except Exception as e:
        logger.error(f"èŒä½æ¨èå¤±è´¥: {e}")
        return sanic_response({"error": str(e)}, status=500)
```

#### 3.2 ä¼ä¸šæ¨èAPI
```python
@app.route("/api/v1/recommend/companies", methods=["POST"])
async def recommend_companies(request: Request):
    """ä¸ºç”¨æˆ·æ¨èä¼ä¸š"""
    try:
        data = request.json
        user_id = data.get("user_id")
        limit = data.get("limit", 10)
        filters = data.get("filters", {})
        
        logger.info(f"å¼€å§‹ä¸ºç”¨æˆ·æ¨èä¼ä¸š: {user_id}")
        
        # è·å–ç”¨æˆ·ç”»åƒ
        user_profile = await get_user_profile(user_id)
        
        # æ‰§è¡Œä¼ä¸šæ¨è
        recommendations = await perform_company_recommendation(user_id, user_profile, limit, filters)
        
        # ä¿å­˜æ¨èç»“æœ
        await save_company_recommendations_to_db(user_id, recommendations)
        
        response = {
            "user_id": user_id,
            "recommendations": [
                {
                    "company_id": rec.company_id,
                    "recommendation_score": rec.score,
                    "match_reasons": rec.match_reasons,
                    "match_factors": rec.match_factors
                } for rec in recommendations
            ],
            "total": len(recommendations),
            "generated_at": datetime.now().isoformat()
        }
        
        logger.info(f"ä¼ä¸šæ¨èå®Œæˆ: {user_id}, æ¨èæ•°é‡: {len(recommendations)}")
        return sanic_response(response)
        
    except Exception as e:
        logger.error(f"ä¼ä¸šæ¨èå¤±è´¥: {e}")
        return sanic_response({"error": str(e)}, status=500)
```

### 4. AIå¯¹è¯æœåŠ¡æ¥å…¥å®ç°

#### 4.1 æ™ºèƒ½å¯¹è¯API
```python
@app.route("/api/v1/chat", methods=["POST"])
async def ai_chat(request: Request):
    """AIæ™ºèƒ½å¯¹è¯"""
    try:
        data = request.json
        user_id = data.get("user_id")
        message = data.get("message")
        conversation_type = data.get("conversation_type", "general")
        session_id = data.get("session_id")
        
        logger.info(f"å¼€å§‹AIå¯¹è¯: {user_id}, ç±»å‹: {conversation_type}")
        
        # è·å–æˆ–åˆ›å»ºå¯¹è¯ä¼šè¯
        conversation = await get_or_create_conversation(user_id, session_id, conversation_type)
        
        # æ‰§è¡ŒAIå¯¹è¯
        response = await perform_ai_chat(conversation, message)
        
        # ä¿å­˜å¯¹è¯è®°å½•
        await save_chat_message(conversation.id, "user", message)
        await save_chat_message(conversation.id, "assistant", response.content)
        
        return sanic_response({
            "conversation_id": conversation.id,
            "session_id": session_id,
            "response": response.content,
            "metadata": response.metadata,
            "tokens_used": response.tokens_used,
            "processing_time_ms": response.processing_time_ms,
            "created_at": datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"AIå¯¹è¯å¤±è´¥: {e}")
        return sanic_response({"error": str(e)}, status=500)
```

#### 4.2 å¯¹è¯æ•°æ®æ¨¡å‹
```python
class ChatRequest:
    def __init__(self, user_id: int, message: str, conversation_type: str, session_id: str):
        self.user_id = user_id
        self.message = message
        self.conversation_type = conversation_type
        self.session_id = session_id

class ChatResponse:
    def __init__(self, content: str, metadata: Dict[str, Any], 
                 tokens_used: int, processing_time_ms: int):
        self.content = content
        self.metadata = metadata
        self.tokens_used = tokens_used
        self.processing_time_ms = processing_time_ms

class Conversation:
    def __init__(self, id: int, user_id: int, conversation_type: str, 
                 session_id: str, context_data: Dict[str, Any]):
        self.id = id
        self.user_id = user_id
        self.conversation_type = conversation_type
        self.session_id = session_id
        self.context_data = context_data
```

### 5. æ•°æ®åº“é›†æˆå®ç°

#### 5.1 ä¼ä¸šåˆ†ææ•°æ®ä¿å­˜
```python
async def save_company_analysis_to_db(company_id: int, analysis: CompanyAnalysis, embeddings: CompanyEmbeddings):
    """ä¿å­˜ä¼ä¸šåˆ†æç»“æœåˆ°æ•°æ®åº“"""
    conn = get_db_connection()
    if not conn:
        raise Exception("æ•°æ®åº“è¿æ¥å¤±è´¥")
    
    try:
        with conn.cursor() as cursor:
            # ä¿å­˜ä¼ä¸šAIç”»åƒ
            cursor.execute("""
                INSERT INTO company_ai_profiles 
                (company_id, profile_type, profile_data, confidence_score, generated_at, expires_at, is_valid)
                VALUES (%s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (company_id, profile_type) 
                DO UPDATE SET 
                    profile_data = EXCLUDED.profile_data,
                    confidence_score = EXCLUDED.confidence_score,
                    generated_at = EXCLUDED.generated_at,
                    expires_at = EXCLUDED.expires_at
            """, (
                company_id,
                'comprehensive',
                json.dumps({
                    'company_profile': analysis.company_profile,
                    'culture_analysis': analysis.culture_analysis,
                    'benefits_analysis': analysis.benefits_analysis,
                    'growth_potential': analysis.growth_potential,
                    'industry_position': analysis.industry_position
                }),
                analysis.confidence_score,
                datetime.now(),
                datetime.now() + timedelta(days=30),
                True
            ))
            
            # ä¿å­˜ä¼ä¸šåµŒå…¥å‘é‡
            for embedding_type, vector in [
                ('description', embeddings.description_vector),
                ('culture', embeddings.culture_vector),
                ('benefits', embeddings.benefits_vector),
                ('overall', embeddings.overall_vector)
            ]:
                cursor.execute("""
                    INSERT INTO company_embeddings 
                    (company_id, embedding_type, embedding_vector, model_id, created_at)
                    VALUES (%s, %s, %s, %s, %s)
                    ON CONFLICT (company_id, embedding_type) 
                    DO UPDATE SET 
                        embedding_vector = EXCLUDED.embedding_vector,
                        model_id = EXCLUDED.model_id,
                        created_at = EXCLUDED.created_at
                """, (
                    company_id,
                    embedding_type,
                    json.dumps(vector),
                    1,  # å‡è®¾æ¨¡å‹IDä¸º1
                    datetime.now()
                ))
            
            conn.commit()
            logger.info(f"ä¼ä¸šåˆ†ææ•°æ®å·²ä¿å­˜åˆ°æ•°æ®åº“: {company_id}")
            
    except Exception as e:
        conn.rollback()
        logger.error(f"ä¿å­˜ä¼ä¸šåˆ†ææ•°æ®å¤±è´¥: {e}")
        raise
    finally:
        conn.close()
```

#### 5.2 èŒä½åˆ†ææ•°æ®ä¿å­˜
```python
async def save_job_analysis_to_db(job_id: int, analysis: JobAnalysis, embeddings: JobEmbeddings):
    """ä¿å­˜èŒä½åˆ†æç»“æœåˆ°æ•°æ®åº“"""
    conn = get_db_connection()
    if not conn:
        raise Exception("æ•°æ®åº“è¿æ¥å¤±è´¥")
    
    try:
        with conn.cursor() as cursor:
            # ä¿å­˜èŒä½AIåˆ†æ
            cursor.execute("""
                INSERT INTO job_ai_analysis 
                (job_id, analysis_type, analysis_result, confidence_score, generated_at, expires_at, is_valid)
                VALUES (%s, %s, %s, %s, %s, %s, %s)
            """, (
                job_id,
                'comprehensive',
                json.dumps({
                    'enhanced_description': analysis.enhanced_description,
                    'extracted_skills': analysis.extracted_skills,
                    'salary_prediction': analysis.salary_prediction,
                    'experience_requirements': analysis.experience_requirements,
                    'company_culture_fit': analysis.company_culture_fit
                }),
                analysis.confidence_score,
                datetime.now(),
                datetime.now() + timedelta(days=7),
                True
            ))
            
            # ä¿å­˜èŒä½åµŒå…¥å‘é‡
            for embedding_type, vector in [
                ('title', embeddings.title_vector),
                ('description', embeddings.description_vector),
                ('requirements', embeddings.requirements_vector),
                ('overall', embeddings.overall_vector)
            ]:
                cursor.execute("""
                    INSERT INTO job_embeddings 
                    (job_id, embedding_type, embedding_vector, model_id, created_at)
                    VALUES (%s, %s, %s, %s, %s)
                    ON CONFLICT (job_id, embedding_type) 
                    DO UPDATE SET 
                        embedding_vector = EXCLUDED.embedding_vector,
                        model_id = EXCLUDED.model_id,
                        created_at = EXCLUDED.created_at
                """, (
                    job_id,
                    embedding_type,
                    json.dumps(vector),
                    1,  # å‡è®¾æ¨¡å‹IDä¸º1
                    datetime.now()
                ))
            
            conn.commit()
            logger.info(f"èŒä½åˆ†ææ•°æ®å·²ä¿å­˜åˆ°æ•°æ®åº“: {job_id}")
            
    except Exception as e:
        conn.rollback()
        logger.error(f"ä¿å­˜èŒä½åˆ†ææ•°æ®å¤±è´¥: {e}")
        raise
    finally:
        conn.close()
```

### 6. é…ç½®å’Œéƒ¨ç½²

#### 6.1 ç¯å¢ƒé…ç½®æ›´æ–°
```python
# åœ¨Configç±»ä¸­æ·»åŠ æ–°çš„é…ç½®é¡¹
class Config:
    PORT = int(os.getenv("AI_SERVICE_PORT", 8206))
    POSTGRES_HOST = os.getenv("POSTGRES_HOST", "localhost")
    POSTGRES_USER = os.getenv("POSTGRES_USER", "szjason72")
    POSTGRES_DB = os.getenv("POSTGRES_DB", "jobfirst_vector")
    POSTGRES_PASSWORD = os.getenv("POSTGRES_PASSWORD", "")
    OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://127.0.0.1:11434")
    OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "gemma3:4b")
    
    # æ–°å¢é…ç½®
    MYSQL_HOST = os.getenv("MYSQL_HOST", "localhost")
    MYSQL_USER = os.getenv("MYSQL_USER", "root")
    MYSQL_DB = os.getenv("MYSQL_DB", "jobfirst")
    MYSQL_PASSWORD = os.getenv("MYSQL_PASSWORD", "")
    
    # AIæœåŠ¡é…ç½®
    AI_CACHE_TTL = int(os.getenv("AI_CACHE_TTL", 3600))  # ç¼“å­˜è¿‡æœŸæ—¶é—´
    AI_MAX_TOKENS = int(os.getenv("AI_MAX_TOKENS", 2000))  # æœ€å¤§tokenæ•°
    AI_TEMPERATURE = float(os.getenv("AI_TEMPERATURE", 0.3))  # æ¸©åº¦å‚æ•°
```

#### 6.2 æ•°æ®åº“è¿æ¥æ± 
```python
import mysql.connector
from mysql.connector import pooling

# MySQLè¿æ¥æ± é…ç½®
mysql_config = {
    'host': Config.MYSQL_HOST,
    'user': Config.MYSQL_USER,
    'password': Config.MYSQL_PASSWORD,
    'database': Config.MYSQL_DB,
    'pool_name': 'jobfirst_pool',
    'pool_size': 10,
    'pool_reset_session': True
}

# åˆ›å»ºè¿æ¥æ± 
mysql_pool = mysql.connector.pooling.MySQLConnectionPool(**mysql_config)

def get_mysql_connection():
    """è·å–MySQLæ•°æ®åº“è¿æ¥"""
    try:
        return mysql_pool.get_connection()
    except Exception as e:
        logger.error(f"MySQLè¿æ¥å¤±è´¥: {e}")
        return None
```

### 7. APIæµ‹è¯•è„šæœ¬

#### 7.1 ä¼ä¸šåˆ†æAPIæµ‹è¯•
```bash
#!/bin/bash
# test_company_analysis.sh

echo "ğŸ§ª æµ‹è¯•ä¼ä¸šåˆ†æAPI"
echo "=================="

BASE_URL="http://localhost:8206"

# æµ‹è¯•ä¼ä¸šåˆ†æ
echo "1. æµ‹è¯•ä¼ä¸šåˆ†æ..."
curl -X POST "$BASE_URL/api/v1/analyze/company" \
  -H "Content-Type: application/json" \
  -d '{
    "company_id": 1,
    "company_data": {
      "name": "è…¾è®¯ç§‘æŠ€",
      "industry": "äº’è”ç½‘",
      "size": "enterprise",
      "location": "æ·±åœ³",
      "description": "ä¸­å›½é¢†å…ˆçš„äº’è”ç½‘ç»¼åˆæœåŠ¡æä¾›å•†",
      "website": "https://www.tencent.com"
    }
  }' | jq '.'
echo ""

# æµ‹è¯•ä¼ä¸šæ¨è
echo "2. æµ‹è¯•ä¼ä¸šæ¨è..."
curl -X POST "$BASE_URL/api/v1/recommend/companies" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": 1,
    "limit": 5,
    "filters": {
      "industry": "äº’è”ç½‘",
      "location": "æ·±åœ³"
    }
  }' | jq '.'
echo ""
```

#### 7.2 èŒä½åˆ†æAPIæµ‹è¯•
```bash
#!/bin/bash
# test_job_analysis.sh

echo "ğŸ§ª æµ‹è¯•èŒä½åˆ†æAPI"
echo "=================="

BASE_URL="http://localhost:8206"

# æµ‹è¯•èŒä½åˆ†æ
echo "1. æµ‹è¯•èŒä½åˆ†æ..."
curl -X POST "$BASE_URL/api/v1/analyze/job" \
  -H "Content-Type: application/json" \
  -d '{
    "job_id": 1,
    "job_data": {
      "title": "å‰ç«¯å¼€å‘å·¥ç¨‹å¸ˆ",
      "company": "è…¾è®¯ç§‘æŠ€",
      "location": "æ·±åœ³",
      "description": "è´Ÿè´£å…¬å¸æ ¸å¿ƒäº§å“çš„å‰ç«¯å¼€å‘å·¥ä½œ",
      "requirements": "ç†Ÿæ‚‰Reactã€Vueç­‰å‰ç«¯æ¡†æ¶",
      "salary_min": 15000,
      "salary_max": 25000
    }
  }' | jq '.'
echo ""

# æµ‹è¯•èŒä½æ¨è
echo "2. æµ‹è¯•èŒä½æ¨è..."
curl -X POST "$BASE_URL/api/v1/recommend/jobs" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": 1,
    "limit": 5,
    "filters": {
      "location": "æ·±åœ³",
      "salary_min": 10000
    }
  }' | jq '.'
echo ""
```

#### 7.3 AIå¯¹è¯APIæµ‹è¯•
```bash
#!/bin/bash
# test_ai_chat.sh

echo "ğŸ§ª æµ‹è¯•AIå¯¹è¯API"
echo "================"

BASE_URL="http://localhost:8206"

# æµ‹è¯•AIå¯¹è¯
echo "1. æµ‹è¯•AIå¯¹è¯..."
curl -X POST "$BASE_URL/api/v1/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": 1,
    "message": "è¯·å¸®æˆ‘åˆ†æä¸€ä¸‹æˆ‘çš„ç®€å†ï¼Œæœ‰ä»€ä¹ˆéœ€è¦æ”¹è¿›çš„åœ°æ–¹å—ï¼Ÿ",
    "conversation_type": "resume_review",
    "session_id": "test_session_001"
  }' | jq '.'
echo ""

# æµ‹è¯•èŒä¸šå»ºè®®
echo "2. æµ‹è¯•èŒä¸šå»ºè®®..."
curl -X POST "$BASE_URL/api/v1/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": 1,
    "message": "æˆ‘æƒ³è½¬è¡Œåšå‰ç«¯å¼€å‘ï¼Œéœ€è¦å­¦ä¹ å“ªäº›æŠ€èƒ½ï¼Ÿ",
    "conversation_type": "career_advice",
    "session_id": "test_session_002"
  }' | jq '.'
echo ""
```

## ğŸ¯ å®æ–½æ­¥éª¤

### é˜¶æ®µä¸€ï¼šåŸºç¡€æ¶æ„æ‰©å±• (1å‘¨)
1. **æ‰©å±•ç°æœ‰AIæœåŠ¡**
   - æ·»åŠ ä¼ä¸šåˆ†æAPI
   - æ·»åŠ èŒä½åˆ†æAPI
   - æ›´æ–°æ•°æ®æ¨¡å‹

2. **æ•°æ®åº“é›†æˆ**
   - è¿æ¥MySQLæ•°æ®åº“
   - å®ç°æ•°æ®ä¿å­˜å’ŒæŸ¥è¯¢
   - æ·»åŠ è¿æ¥æ± 

### é˜¶æ®µäºŒï¼šæ™ºèƒ½æ¨èæœåŠ¡ (1å‘¨)
1. **æ¨èç®—æ³•å®ç°**
   - èŒä½æ¨èç®—æ³•
   - ä¼ä¸šæ¨èç®—æ³•
   - ç›¸ä¼¼åº¦è®¡ç®—

2. **ç”¨æˆ·ç”»åƒæ„å»º**
   - åŸºäºç®€å†æ„å»ºç”¨æˆ·ç”»åƒ
   - æŠ€èƒ½åŒ¹é…åˆ†æ
   - åå¥½å­¦ä¹ 

### é˜¶æ®µä¸‰ï¼šAIå¯¹è¯æœåŠ¡ (1å‘¨)
1. **å¯¹è¯ç³»ç»Ÿå®ç°**
   - æ™ºèƒ½é—®ç­”
   - èŒä¸šå’¨è¯¢
   - ç®€å†ä¼˜åŒ–å»ºè®®

2. **ä¸Šä¸‹æ–‡ç®¡ç†**
   - å¯¹è¯ä¼šè¯ç®¡ç†
   - ä¸Šä¸‹æ–‡ä¿æŒ
   - ä¸ªæ€§åŒ–å“åº”

### é˜¶æ®µå››ï¼šæµ‹è¯•å’Œä¼˜åŒ– (1å‘¨)
1. **APIæµ‹è¯•**
   - å•å…ƒæµ‹è¯•
   - é›†æˆæµ‹è¯•
   - æ€§èƒ½æµ‹è¯•

2. **æ€§èƒ½ä¼˜åŒ–**
   - ç¼“å­˜æœºåˆ¶
   - å¼‚æ­¥å¤„ç†
   - é”™è¯¯å¤„ç†

## ğŸ“Š é¢„æœŸæ•ˆæœ

### 1. åŠŸèƒ½å¢å¼º
- **ä¼ä¸šæ™ºèƒ½åˆ†æ**: æ·±åº¦ä¼ä¸šç”»åƒå’Œè¡Œä¸šåˆ†æ
- **èŒä½æ™ºèƒ½åŒ¹é…**: ç²¾å‡†çš„èŒä½æ¨èå’ŒåŒ¹é…
- **AIå¯¹è¯æœåŠ¡**: 24/7çš„æ™ºèƒ½èŒä¸šå’¨è¯¢æœåŠ¡
- **ä¸ªæ€§åŒ–æ¨è**: åŸºäºç”¨æˆ·ç”»åƒçš„æ™ºèƒ½æ¨è

### 2. æŠ€æœ¯æå‡
- **APIæ‰©å±•**: ä»4ä¸ªAPIæ‰©å±•åˆ°15+ä¸ªAPI
- **æ•°æ®åº“é›†æˆ**: æ”¯æŒPostgreSQLå’ŒMySQLåŒæ•°æ®åº“
- **æ¨¡å‹ç®¡ç†**: å®Œæ•´çš„AIæ¨¡å‹ç‰ˆæœ¬ç®¡ç†
- **ç›‘æ§ä½“ç³»**: å®Œæ•´çš„AIæœåŠ¡ç›‘æ§å’Œæ—¥å¿—

### 3. ç”¨æˆ·ä½“éªŒ
- **æ™ºèƒ½æ¨è**: æ›´ç²¾å‡†çš„èŒä½å’Œä¼ä¸šæ¨è
- **èŒä¸šæŒ‡å¯¼**: AIé©±åŠ¨çš„èŒä¸šå‘å±•å»ºè®®
- **å®æ—¶åé¦ˆ**: å³æ—¶çš„ç®€å†å’ŒæŠ€èƒ½åˆ†æ
- **ä¸ªæ€§åŒ–æœåŠ¡**: åŸºäºç”¨æˆ·ç”»åƒçš„ä¸ªæ€§åŒ–ä½“éªŒ

## ğŸ‰ æ€»ç»“

æœ¬å®ç°æŒ‡å—åŸºäºç°æœ‰çš„Python Sanic AIæœåŠ¡æ¶æ„ï¼Œæä¾›äº†å®Œæ•´çš„ä¼ä¸šå’ŒèŒä½AIæœåŠ¡æ¥å…¥æ–¹æ¡ˆã€‚é€šè¿‡æ‰©å±•ç°æœ‰æœåŠ¡ï¼ŒJobFirstå°†è·å¾—ï¼š

1. **å®Œæ•´çš„ä¼ä¸šAIåˆ†æèƒ½åŠ›**
2. **ç²¾å‡†çš„èŒä½æ™ºèƒ½åŒ¹é…**
3. **æ™ºèƒ½çš„AIå¯¹è¯æœåŠ¡**
4. **ä¸ªæ€§åŒ–çš„æ¨èç³»ç»Ÿ**

è¿™äº›åŠŸèƒ½å°†æ˜¾è‘—æå‡JobFirstå¹³å°çš„æ™ºèƒ½åŒ–æ°´å¹³ï¼Œä¸ºç”¨æˆ·æä¾›æ›´ä¼˜è´¨çš„èŒä¸šå‘å±•æœåŠ¡ã€‚

---

**å®ç°æŒ‡å—å®Œæˆæ—¶é—´**: 2025å¹´1æœˆ6æ—¥ 11:30  
**å®ç°çŠ¶æ€**: å®Œæˆ  
**ä¸‹ä¸€æ­¥**: å¼€å§‹å®æ–½AIæœåŠ¡æ‰©å±•
